{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import re\n",
    "import contractions\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from itertools import chain\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "## Approx run time - 2s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\shubh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\shubh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\shubh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\shubh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "## Approx run time - 1s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_dets = pd.read_csv(\"promapen-train_data.csv\")\n",
    "\n",
    "## Approx run time - 1 min 10 s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_dets['text1'] = product_dets['name1'] + \" \" + product_dets['short_description1'] + \" \" + product_dets['long_description1'] + \" \" + product_dets['specification1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_dets['text2'] = product_dets['name2'] + \" \" + product_dets['short_description2'] + \" \" + product_dets['long_description2'] + \" \" + product_dets['specification2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_clean(s:str):\n",
    "    s = BeautifulSoup(str(s)).get_text() #removing HTML and XML tags\n",
    "    s = re.split(\" |\\,|;|\\.|:\",s) #splitting on basis of spaces, commas, colon, semi-colon and periods\n",
    "    s = [contractions.fix(x).lower().strip().split() for x in s] #expanding contractions and changing text to lower case, removing extra spaces\n",
    "    s = list(chain(*s)) #Flattening out the array\n",
    "\n",
    "    #Keeping the Alpha Numeric characters and spaces\n",
    "    s = (\" \").join([re.sub(r'[^A-Za-z0-9\\s]+', ' ', word) for word in s])\n",
    "\n",
    "    #removing extra spaces\n",
    "    s = (\" \").join(s.split()).strip()\n",
    "\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shubh\\AppData\\Local\\Temp\\ipykernel_2228\\3026544751.py:2: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  s = BeautifulSoup(str(s)).get_text() #removing HTML and XML tags\n"
     ]
    }
   ],
   "source": [
    "product_dets['text1'] = product_dets['text1'].apply(data_clean)\n",
    "product_dets['text2'] = product_dets['text2'].apply(data_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1601.1093247588424"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "product_dets['text1'].apply(str).apply(len).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1335.8609324758843"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "product_dets['text2'].apply(str).apply(len).mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stop(s:str):\n",
    "    stopw = set(stopwords.words(\"english\")) #creating set of stop words from NLTK\n",
    "    tokens = nltk.tokenize.word_tokenize(s) # generating tokens\n",
    "    tokens = [x for x in tokens if x not in stopw] # removing the stop words\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "def custom_lemmatize(s:list):\n",
    "    pos_tagged = nltk.pos_tag(s) #pos tagging with nltk\n",
    "    #lemmatizing by using the pos tags\n",
    "    tokens = [wnl.lemmatize(x[0]) for x in pos_tagged] \n",
    "    return \" \".join(tokens) #returning in sentence format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_dets['text1'] = product_dets['text1'].apply(remove_stop)\n",
    "product_dets['text2'] = product_dets['text2'].apply(remove_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_dets['text1'] = product_dets['text1'].apply(custom_lemmatize)\n",
    "product_dets['text2'] = product_dets['text2'].apply(custom_lemmatize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    bagcraft p057012 12 x 12 grease resistant pape...\n",
       "1    clorox 35420 128 oz clean disinfectant cleaner...\n",
       "2    clorox 35420 128 oz clean disinfectant cleaner...\n",
       "3    2 pack lysol disinfecting wipe lemon lime blos...\n",
       "4    2 pack lysol disinfecting wipe lemon lime blos...\n",
       "Name: text1, dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "product_dets['text1'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer1 = TfidfVectorizer(stop_words='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF feature extraction\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\"\"\"\n",
    "    1. reducing the size to float 32 to avoid memory issues - dtype = float32\n",
    "    2. using ngram_range to consider 1 to 4 words together while extracting the features\n",
    "\"\"\"\n",
    "tfidf = TfidfVectorizer(dtype = np.float32, min_df=2,ngram_range=(1,4))\n",
    "tfidf = tfidf.fit(product_dets['text1'])\n",
    "tfidf = tfidf.fit(product_dets['text2'])\n",
    "\n",
    "tfidf_vectors_text1 = tfidf.transform(product_dets['text1'])\n",
    "tfidf_vectors_text2 = tfidf.transform(product_dets['text2'])\n",
    "## Approx run time - 30s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x74689 sparse matrix of type '<class 'numpy.float32'>'\n",
       "\twith 164 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vectors_text2[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x74689 sparse matrix of type '<class 'numpy.float32'>'\n",
       "\twith 77 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vectors_text1[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1880286"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(tfidf_vectors_text2[0].toarray(),tfidf_vectors_text1[0].toarray().T)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09403747"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(tfidf_vectors_text2[0].toarray(),tfidf_vectors_text1[9].toarray().T)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., ..., 0., 0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vectors_text2[0].toarray()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9718383252620697"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.spatial import distance\n",
    "\n",
    "distance.cosine(tfidf_vectors_text2[0].toarray()[0],tfidf_vectors_text1[3].toarray()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1244, 74689)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vectors_text1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "top1 = 0\n",
    "top5 = 0\n",
    "for i in range(tfidf_vectors_text1.shape[0]):\n",
    "    list_dist = np.array([])\n",
    "    for j in range(tfidf_vectors_text2.shape[0]):\n",
    "        val = np.dot(tfidf_vectors_text1[i].toarray(),tfidf_vectors_text2[j].toarray().T)[0][0]\n",
    "        list_dist =np.append(list_dist,val)\n",
    "\n",
    "    maxI = list_dist.argsort()[::-1][:10]\n",
    "\n",
    "    if maxI[0] == i:\n",
    "        top1+=1\n",
    "        top5+=1\n",
    "    elif i in maxI:\n",
    "        top5+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "top1 = 0\n",
    "top5 = 0\n",
    "for i in range(tfidf_vectors_text1.shape[0]):\n",
    "    list_dist = np.array([])\n",
    "    for j in range(tfidf_vectors_text2.shape[0]):\n",
    "        val = distance.cosine(tfidf_vectors_text1[i].toarray()[0],tfidf_vectors_text2[j].toarray()[0])\n",
    "        list_dist =np.append(list_dist,val)\n",
    "\n",
    "    maxI = list_dist.argsort()[:10]\n",
    "\n",
    "    if maxI[0] == i:\n",
    "        top1+=1\n",
    "        top5+=1\n",
    "    elif i in maxI:\n",
    "        top5+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct match - 0.3215434083601286\n",
      "Top 5 match - 0.6583601286173634\n"
     ]
    }
   ],
   "source": [
    "l = tfidf_vectors_text1.shape[0]\n",
    "\n",
    "print(\"Correct match - \"+ str(top1/l))\n",
    "print(\"Top  match 5 - \"+ str(top5/l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1186,  895,    0,   59,   58], dtype=int64)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_dist.argsort()[::-1][:5]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "567Project-7k1jFqDZ",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
