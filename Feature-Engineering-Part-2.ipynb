{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9726bcee",
   "metadata": {},
   "source": [
    "# Feaure Engineering - Text Processing\n",
    "\n",
    "* Process the textual data\n",
    "* Combine the hash similarities of the images (Feature-Engineering-Part-1)\n",
    "* Save the features to CSV files in the \"features/ProMapEn/\" path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce422480",
   "metadata": {},
   "source": [
    "## 1. Modules and Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ccdc76b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\shubh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\shubh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import contractions\n",
    "\n",
    "import nltk\n",
    "nltk.download('words')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import words\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "## Average runtime - 1s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec28a315",
   "metadata": {},
   "source": [
    "## 2. Load data\n",
    "\n",
    "* Combined the train and test for feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6331bba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1555, 21)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name1</th>\n",
       "      <th>short_description1</th>\n",
       "      <th>long_description1</th>\n",
       "      <th>specification1</th>\n",
       "      <th>image1</th>\n",
       "      <th>price1</th>\n",
       "      <th>id1</th>\n",
       "      <th>name2</th>\n",
       "      <th>short_description2</th>\n",
       "      <th>long_description2</th>\n",
       "      <th>...</th>\n",
       "      <th>image2</th>\n",
       "      <th>price2</th>\n",
       "      <th>id2</th>\n",
       "      <th>match</th>\n",
       "      <th>image_url1</th>\n",
       "      <th>image_url2</th>\n",
       "      <th>category</th>\n",
       "      <th>match_type</th>\n",
       "      <th>specification_text1</th>\n",
       "      <th>specification_text2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bagcraft P057012 12 x 12 Grease-Resistant Pape...</td>\n",
       "      <td>Excellent low-cost, low-waste alternative to p...</td>\n",
       "      <td>Wrap/liner is an excellent low-cost, low-waste...</td>\n",
       "      <td>[{\"key\": \"Features\", \"value\": \"Excellent low-c...</td>\n",
       "      <td>3</td>\n",
       "      <td>131.59</td>\n",
       "      <td>https://walmart.com/ip/Bagcraft-P057012-12-x-1...</td>\n",
       "      <td>Bagcraft Papercon 012008 Interfolded Heavy Dry...</td>\n",
       "      <td>Provides wet strength, improved moisture resis...</td>\n",
       "      <td>Bagcraft interfolded heavy dry wax deli paper....</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>135.1</td>\n",
       "      <td>https://www.amazon.com/dp/B00C7KTHHI</td>\n",
       "      <td>0</td>\n",
       "      <td>[\"https://i5.walmartimages.com/asr/8f9b23a7-f4...</td>\n",
       "      <td>[\"https://m.media-amazon.com/images/I/51VDhs3N...</td>\n",
       "      <td>6_household</td>\n",
       "      <td>medium_nonmatch</td>\n",
       "      <td>Features Excellent low-cost, low-waste alterna...</td>\n",
       "      <td>Brand Name Bagcraft Papercon Global Trade Iden...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Clorox 35420 128 oz. Clean-Up Disinfectant Cle...</td>\n",
       "      <td>Removes stains and disinfects to kill 99.9% of...</td>\n",
       "      <td>Clorox Clean-Up CloroxPro Disinfectant Cleaner...</td>\n",
       "      <td>[{\"key\": \"Assembled Product Weight\", \"value\": ...</td>\n",
       "      <td>5</td>\n",
       "      <td>61.38</td>\n",
       "      <td>https://walmart.com/ip/Clorox-35420-128-oz-Cle...</td>\n",
       "      <td>CloroxPro Anywhere Daily Disinfectant and Sani...</td>\n",
       "      <td>NO-RINSE FOOD CONTACT SANITIZER: Confidently s...</td>\n",
       "      <td>CloroxPro Anywhere Daily Disinfectant and Sani...</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>https://www.amazon.com/dp/B07FQRB2XV</td>\n",
       "      <td>0</td>\n",
       "      <td>[\"https://i5.walmartimages.com/asr/3336afe6-d5...</td>\n",
       "      <td>[\"https://m.media-amazon.com/images/I/71f6nNyY...</td>\n",
       "      <td>6_household</td>\n",
       "      <td>close_nonmatch</td>\n",
       "      <td>Assembled Product Weight 37.4 lb Brand Clorox ...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Clorox 35420 128 oz. Clean-Up Disinfectant Cle...</td>\n",
       "      <td>Removes stains and disinfects to kill 99.9% of...</td>\n",
       "      <td>Clorox Clean-Up CloroxPro Disinfectant Cleaner...</td>\n",
       "      <td>[{\"key\": \"Assembled Product Weight\", \"value\": ...</td>\n",
       "      <td>5</td>\n",
       "      <td>61.38</td>\n",
       "      <td>https://walmart.com/ip/Clorox-35420-128-oz-Cle...</td>\n",
       "      <td>CLOROXPRO Commercial Solutions CLOROXPRO Clean...</td>\n",
       "      <td>DISINFECTANT SPRAY: Use this Clorox Clean-Up D...</td>\n",
       "      <td>Clorox Clean-Up Disinfectant Cleaner with Blea...</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>83.6</td>\n",
       "      <td>https://www.amazon.com/dp/B004EHZ7GW</td>\n",
       "      <td>0</td>\n",
       "      <td>[\"https://i5.walmartimages.com/asr/3336afe6-d5...</td>\n",
       "      <td>[\"https://m.media-amazon.com/images/I/81+djgUF...</td>\n",
       "      <td>6_household</td>\n",
       "      <td>medium_nonmatch</td>\n",
       "      <td>Assembled Product Weight 37.4 lb Brand Clorox ...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows Ã— 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               name1  \\\n",
       "0  Bagcraft P057012 12 x 12 Grease-Resistant Pape...   \n",
       "1  Clorox 35420 128 oz. Clean-Up Disinfectant Cle...   \n",
       "2  Clorox 35420 128 oz. Clean-Up Disinfectant Cle...   \n",
       "\n",
       "                                  short_description1  \\\n",
       "0  Excellent low-cost, low-waste alternative to p...   \n",
       "1  Removes stains and disinfects to kill 99.9% of...   \n",
       "2  Removes stains and disinfects to kill 99.9% of...   \n",
       "\n",
       "                                   long_description1  \\\n",
       "0  Wrap/liner is an excellent low-cost, low-waste...   \n",
       "1  Clorox Clean-Up CloroxPro Disinfectant Cleaner...   \n",
       "2  Clorox Clean-Up CloroxPro Disinfectant Cleaner...   \n",
       "\n",
       "                                      specification1  image1  price1  \\\n",
       "0  [{\"key\": \"Features\", \"value\": \"Excellent low-c...       3  131.59   \n",
       "1  [{\"key\": \"Assembled Product Weight\", \"value\": ...       5   61.38   \n",
       "2  [{\"key\": \"Assembled Product Weight\", \"value\": ...       5   61.38   \n",
       "\n",
       "                                                 id1  \\\n",
       "0  https://walmart.com/ip/Bagcraft-P057012-12-x-1...   \n",
       "1  https://walmart.com/ip/Clorox-35420-128-oz-Cle...   \n",
       "2  https://walmart.com/ip/Clorox-35420-128-oz-Cle...   \n",
       "\n",
       "                                               name2  \\\n",
       "0  Bagcraft Papercon 012008 Interfolded Heavy Dry...   \n",
       "1  CloroxPro Anywhere Daily Disinfectant and Sani...   \n",
       "2  CLOROXPRO Commercial Solutions CLOROXPRO Clean...   \n",
       "\n",
       "                                  short_description2  \\\n",
       "0  Provides wet strength, improved moisture resis...   \n",
       "1  NO-RINSE FOOD CONTACT SANITIZER: Confidently s...   \n",
       "2  DISINFECTANT SPRAY: Use this Clorox Clean-Up D...   \n",
       "\n",
       "                                   long_description2  ... image2  price2  \\\n",
       "0  Bagcraft interfolded heavy dry wax deli paper....  ...      1   135.1   \n",
       "1  CloroxPro Anywhere Daily Disinfectant and Sani...  ...      1           \n",
       "2  Clorox Clean-Up Disinfectant Cleaner with Blea...  ...      1    83.6   \n",
       "\n",
       "                                    id2 match  \\\n",
       "0  https://www.amazon.com/dp/B00C7KTHHI     0   \n",
       "1  https://www.amazon.com/dp/B07FQRB2XV     0   \n",
       "2  https://www.amazon.com/dp/B004EHZ7GW     0   \n",
       "\n",
       "                                          image_url1  \\\n",
       "0  [\"https://i5.walmartimages.com/asr/8f9b23a7-f4...   \n",
       "1  [\"https://i5.walmartimages.com/asr/3336afe6-d5...   \n",
       "2  [\"https://i5.walmartimages.com/asr/3336afe6-d5...   \n",
       "\n",
       "                                          image_url2     category  \\\n",
       "0  [\"https://m.media-amazon.com/images/I/51VDhs3N...  6_household   \n",
       "1  [\"https://m.media-amazon.com/images/I/71f6nNyY...  6_household   \n",
       "2  [\"https://m.media-amazon.com/images/I/81+djgUF...  6_household   \n",
       "\n",
       "        match_type                                specification_text1  \\\n",
       "0  medium_nonmatch  Features Excellent low-cost, low-waste alterna...   \n",
       "1   close_nonmatch  Assembled Product Weight 37.4 lb Brand Clorox ...   \n",
       "2  medium_nonmatch  Assembled Product Weight 37.4 lb Brand Clorox ...   \n",
       "\n",
       "                                 specification_text2  \n",
       "0  Brand Name Bagcraft Papercon Global Trade Iden...  \n",
       "1                                                     \n",
       "2                                                     \n",
       "\n",
       "[3 rows x 21 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "promapen_train = pd.read_csv(\"datasets\\\\ProMapEn\\\\promapen-train_data.csv\")\n",
    "promapen_test = pd.read_csv(\"datasets\\\\ProMapEn\\\\promapen-test_data.csv\")\n",
    "\n",
    "promapen = pd.concat([promapen_train, promapen_test], ignore_index=True)\n",
    "promapen = promapen.fillna(\" \")\n",
    "\n",
    "print(promapen.shape)\n",
    "promapen.head(3)\n",
    "\n",
    "## Average runtime - 1s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caecde55",
   "metadata": {},
   "source": [
    "## 3. Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2588e7b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocess:\n",
    "    \"\"\"Preprocess the text\"\"\"\n",
    "    \n",
    "    def __init__(self, dataframe=None):\n",
    "        \"\"\"\n",
    "        Preprocess the data to extract features\n",
    "        Args:\n",
    "            dataframe (pd.Dataframe): data to be processed \n",
    "        \"\"\"\n",
    "        self.dataframe = dataframe\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        self.stopwords = stopwords.words('english')\n",
    "    \n",
    "    def process_brand_names(self, column):\n",
    "        \"\"\"\n",
    "        Extract the brand names from the specified column\n",
    "        Args:\n",
    "            column (str): name of the column\n",
    "        \n",
    "        Return:\n",
    "            list: brand names\n",
    "        \"\"\"\n",
    "        return [\n",
    "            item[\"value\"].lower()\n",
    "            for row in self.dataframe[column]\n",
    "            for item in eval(row)\n",
    "            if item[\"key\"]==\"Brand\" \n",
    "        ]\n",
    "    \n",
    "    def process_text_column(self, column):\n",
    "        \"\"\"\n",
    "        Process the text from the specified column\n",
    "        Args:\n",
    "            column (str): name of the column\n",
    "        \n",
    "        Return:\n",
    "            list: cleaned data\n",
    "        \"\"\"\n",
    "        \n",
    "        processed_values = []\n",
    "        \n",
    "        for text in self.dataframe[column]:\n",
    "            text = contractions.fix(text).lower() \n",
    "            text = re.sub(r\"[^\\w\\s]\", \" \", text) # remove useless characters\n",
    "            text = re.sub(r\"(\\d)([A-Za-z])\", r\"\\1 \\2\", text) # separate units and values\n",
    "\n",
    "            words = [word.strip() for word in word_tokenize(text)] \n",
    "            words = [word for word in words if word not in self.stopwords] # remove stopwords\n",
    "            words = [self.lemmatizer.lemmatize(word.lower()) for word in words] # lemmatize\n",
    "            \n",
    "            processed_values.append(\" \".join(words))\n",
    "        \n",
    "        return processed_values\n",
    "\n",
    "    def process_specification(self, column):\n",
    "        \"\"\"\n",
    "        Process the specification column of the products\n",
    "        Args:\n",
    "            column (str): name of the column\n",
    "            \n",
    "        Return:\n",
    "            list: string format after evaluating the row\n",
    "        \"\"\"\n",
    "        return [\n",
    "            ' '.join([f\"{item['key']} {item['value']}\" for item in eval(row)])\n",
    "            for row in self.dataframe[column]\n",
    "        ]\n",
    "\n",
    "## Average runtime - 1s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ec15eda",
   "metadata": {},
   "source": [
    "### 3.1 Process the dataframe\n",
    "\n",
    "* Store only the necessary columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c1b00ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_processor = Preprocess(promapen)\n",
    "processed_df = pd.DataFrame()\n",
    "\n",
    "# process name, short description and long description\n",
    "processed_df[\"name1\"] = text_processor.process_text_column(\"name1\")\n",
    "processed_df[\"short_description1\"] = text_processor.process_text_column(\"short_description1\")\n",
    "processed_df[\"long_description1\"] = text_processor.process_text_column(\"long_description1\")\n",
    "\n",
    "processed_df[\"name2\"] = text_processor.process_text_column(\"name2\")\n",
    "processed_df[\"short_description2\"] = text_processor.process_text_column(\"short_description2\")\n",
    "processed_df[\"long_description2\"] = text_processor.process_text_column(\"long_description2\")\n",
    "\n",
    "# process the specifications column\n",
    "processed_df[\"specification1\"] = text_processor.process_specification(\"specification1\")\n",
    "processed_df[\"specification2\"] = text_processor.process_specification(\"specification2\")\n",
    "\n",
    "# add all_texts column by combining values from name, short description and long description\n",
    "processed_df[\"all_texts1\"] = processed_df.apply(lambda row: \" \".join([row['name1'], row['short_description1'], row[\"long_description2\"], row[\"specification1\"]]), axis=1)\n",
    "processed_df[\"all_texts2\"] = processed_df.apply(lambda row: \" \".join([row['name2'], row['short_description2'], row[\"long_description2\"], row[\"specification2\"]]), axis=1)\n",
    "\n",
    "# add original specifications\n",
    "processed_df[\"orig_specification1\"] = promapen[\"specification1\"]\n",
    "processed_df[\"orig_specification2\"] = promapen[\"specification2\"]\n",
    "\n",
    "# attach the match_type column\n",
    "processed_df[\"match\"] = promapen[\"match\"]\n",
    "\n",
    "## Average runtime - 8s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "27d97355",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1555, 13)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name1</th>\n",
       "      <th>short_description1</th>\n",
       "      <th>long_description1</th>\n",
       "      <th>name2</th>\n",
       "      <th>short_description2</th>\n",
       "      <th>long_description2</th>\n",
       "      <th>specification1</th>\n",
       "      <th>specification2</th>\n",
       "      <th>all_texts1</th>\n",
       "      <th>all_texts2</th>\n",
       "      <th>orig_specification1</th>\n",
       "      <th>orig_specification2</th>\n",
       "      <th>match</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bagcraft p057012 12 x 12 grease resistant pape...</td>\n",
       "      <td>excellent low cost low waste alternative paper...</td>\n",
       "      <td>wrap liner excellent low cost low waste altern...</td>\n",
       "      <td>bagcraft papercon 012008 interfolded heavy dry...</td>\n",
       "      <td>provides wet strength improved moisture resist...</td>\n",
       "      <td>bagcraft interfolded heavy dry wax deli paper ...</td>\n",
       "      <td>Features Excellent low-cost, low-waste alterna...</td>\n",
       "      <td>Brand Name Bagcraft Papercon Global Trade Iden...</td>\n",
       "      <td>bagcraft p057012 12 x 12 grease resistant pape...</td>\n",
       "      <td>bagcraft papercon 012008 interfolded heavy dry...</td>\n",
       "      <td>[{\"key\": \"Features\", \"value\": \"Excellent low-c...</td>\n",
       "      <td>[{\"key\": \"Brand Name\", \"value\": \"Bagcraft Pape...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>clorox 35420 128 oz clean disinfectant cleaner...</td>\n",
       "      <td>remove stain disinfects kill 99 9 virus bacter...</td>\n",
       "      <td>clorox clean cloroxpro disinfectant cleaner bl...</td>\n",
       "      <td>cloroxpro anywhere daily disinfectant sanitizi...</td>\n",
       "      <td>rinse food contact sanitizer confidently sanit...</td>\n",
       "      <td>cloroxpro anywhere daily disinfectant sanitizi...</td>\n",
       "      <td>Assembled Product Weight 37.4 lb Brand Clorox ...</td>\n",
       "      <td></td>\n",
       "      <td>clorox 35420 128 oz clean disinfectant cleaner...</td>\n",
       "      <td>cloroxpro anywhere daily disinfectant sanitizi...</td>\n",
       "      <td>[{\"key\": \"Assembled Product Weight\", \"value\": ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>clorox 35420 128 oz clean disinfectant cleaner...</td>\n",
       "      <td>remove stain disinfects kill 99 9 virus bacter...</td>\n",
       "      <td>clorox clean cloroxpro disinfectant cleaner bl...</td>\n",
       "      <td>cloroxpro commercial solution cloroxpro clean ...</td>\n",
       "      <td>disinfectant spray use clorox clean disinfecta...</td>\n",
       "      <td>clorox clean disinfectant cleaner bleach power...</td>\n",
       "      <td>Assembled Product Weight 37.4 lb Brand Clorox ...</td>\n",
       "      <td></td>\n",
       "      <td>clorox 35420 128 oz clean disinfectant cleaner...</td>\n",
       "      <td>cloroxpro commercial solution cloroxpro clean ...</td>\n",
       "      <td>[{\"key\": \"Assembled Product Weight\", \"value\": ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               name1  \\\n",
       "0  bagcraft p057012 12 x 12 grease resistant pape...   \n",
       "1  clorox 35420 128 oz clean disinfectant cleaner...   \n",
       "2  clorox 35420 128 oz clean disinfectant cleaner...   \n",
       "\n",
       "                                  short_description1  \\\n",
       "0  excellent low cost low waste alternative paper...   \n",
       "1  remove stain disinfects kill 99 9 virus bacter...   \n",
       "2  remove stain disinfects kill 99 9 virus bacter...   \n",
       "\n",
       "                                   long_description1  \\\n",
       "0  wrap liner excellent low cost low waste altern...   \n",
       "1  clorox clean cloroxpro disinfectant cleaner bl...   \n",
       "2  clorox clean cloroxpro disinfectant cleaner bl...   \n",
       "\n",
       "                                               name2  \\\n",
       "0  bagcraft papercon 012008 interfolded heavy dry...   \n",
       "1  cloroxpro anywhere daily disinfectant sanitizi...   \n",
       "2  cloroxpro commercial solution cloroxpro clean ...   \n",
       "\n",
       "                                  short_description2  \\\n",
       "0  provides wet strength improved moisture resist...   \n",
       "1  rinse food contact sanitizer confidently sanit...   \n",
       "2  disinfectant spray use clorox clean disinfecta...   \n",
       "\n",
       "                                   long_description2  \\\n",
       "0  bagcraft interfolded heavy dry wax deli paper ...   \n",
       "1  cloroxpro anywhere daily disinfectant sanitizi...   \n",
       "2  clorox clean disinfectant cleaner bleach power...   \n",
       "\n",
       "                                      specification1  \\\n",
       "0  Features Excellent low-cost, low-waste alterna...   \n",
       "1  Assembled Product Weight 37.4 lb Brand Clorox ...   \n",
       "2  Assembled Product Weight 37.4 lb Brand Clorox ...   \n",
       "\n",
       "                                      specification2  \\\n",
       "0  Brand Name Bagcraft Papercon Global Trade Iden...   \n",
       "1                                                      \n",
       "2                                                      \n",
       "\n",
       "                                          all_texts1  \\\n",
       "0  bagcraft p057012 12 x 12 grease resistant pape...   \n",
       "1  clorox 35420 128 oz clean disinfectant cleaner...   \n",
       "2  clorox 35420 128 oz clean disinfectant cleaner...   \n",
       "\n",
       "                                          all_texts2  \\\n",
       "0  bagcraft papercon 012008 interfolded heavy dry...   \n",
       "1  cloroxpro anywhere daily disinfectant sanitizi...   \n",
       "2  cloroxpro commercial solution cloroxpro clean ...   \n",
       "\n",
       "                                 orig_specification1  \\\n",
       "0  [{\"key\": \"Features\", \"value\": \"Excellent low-c...   \n",
       "1  [{\"key\": \"Assembled Product Weight\", \"value\": ...   \n",
       "2  [{\"key\": \"Assembled Product Weight\", \"value\": ...   \n",
       "\n",
       "                                 orig_specification2  match  \n",
       "0  [{\"key\": \"Brand Name\", \"value\": \"Bagcraft Pape...      0  \n",
       "1                                                 []      0  \n",
       "2                                                 []      0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(processed_df.shape)\n",
    "processed_df.head(3)\n",
    "\n",
    "## Average runtime - 1s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_df.to_parquet('Processed.parquet')\n",
    "## Read when required\n",
    "# processed_df = pd.read_parquet('Processed.parquet')\n",
    "\n",
    "## Average runtime - 1s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8579a1f0",
   "metadata": {},
   "source": [
    "## 4. Feature Extraction\n",
    "\n",
    "### 4.1 Text similarity computations\n",
    "\n",
    "* Calculate cosine similarity between the textual information of 2 products\n",
    "* Extracted 4 features: name_cos, short_description_cos, long_description_cos, all_texts_cos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TFIDF_vectorizer:\n",
    "    \n",
    "    \n",
    "    def __init__(self,dataframe_column) -> None:\n",
    "        \"\"\"\n",
    "        Fitting the columns into TfidfVectorizer\n",
    "        Args:\n",
    "            series (pd.Series): data to be fitted \n",
    "        \"\"\"\n",
    "        self.vectorizer = TfidfVectorizer().fit(dataframe_column)\n",
    "    \n",
    "    def transformer(self, text1, text2) -> list:\n",
    "        \"\"\"\n",
    "        Transform the text into tfidf vectors\n",
    "        Args:\n",
    "            text1 (str): column 1 data\n",
    "            text2 (str): column 2 data\n",
    "        \"\"\"\n",
    "        return [self.vectorizer.transform([text1]),self.vectorizer.transform([text2])]\n",
    "    \n",
    "    def calculate_cosine_similarity(self, text1, text2):\n",
    "        \"\"\"\n",
    "        Calculates the cosine distance between the text vectors\n",
    "        Args:\n",
    "            text1 (str): column 1 data\n",
    "            text2 (str): column 2 data\n",
    "        \"\"\"\n",
    "        try:\n",
    "            tfidf_matrix = self.transformer(text1, text2)\n",
    "            return cosine_similarity(tfidf_matrix[0], tfidf_matrix[1])[0][0]\n",
    "        except:\n",
    "            return 0\n",
    "\n",
    "## Average runtime - 1s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a69d40a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_class = TFIDF_vectorizer(pd.concat([processed_df['name1'],\n",
    "                    processed_df['name2'],\n",
    "                    processed_df['short_description1'],\n",
    "                    processed_df['short_description2'],\n",
    "                    processed_df['long_description1'],\n",
    "                    processed_df['long_description2']],ignore_index=True))\n",
    "\n",
    "features_df = pd.DataFrame()\n",
    "\n",
    "# Calculate cosine similarity between name, short and long description columns of 2 products\n",
    "features_df['name_cos'] = processed_df.apply(lambda row: similarity_class.calculate_cosine_similarity(row['name1'], row['name2']), axis=1)\n",
    "features_df['short_description_cos'] = processed_df.apply(lambda row: similarity_class.calculate_cosine_similarity(row['short_description1'], row['short_description2']), axis=1)\n",
    "features_df['long_description_cos'] = processed_df.apply(lambda row: similarity_class.calculate_cosine_similarity(row['long_description1'], row['long_description2']), axis=1)\n",
    "features_df['all_texts_cos'] = processed_df.apply(lambda row: similarity_class.calculate_cosine_similarity(row['all_texts1'], row['all_texts2']), axis=1)\n",
    "\n",
    "## Average runtime - 35s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c1dbba",
   "metadata": {},
   "source": [
    "## 4.2 Keyword Detection and Similarity Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_sim(set1, set2):\n",
    "    \"\"\"\n",
    "    Jaccard similarity between two sets of keywords\n",
    "    Args:\n",
    "        set1 (list): words/tokens\n",
    "        set2 (list): words/tokens\n",
    "    \n",
    "    Return:\n",
    "        float: jaccard similarity\n",
    "    \"\"\"\n",
    "    \n",
    "    if not isinstance(set1, set):\n",
    "        set1 = set(set1)\n",
    "    \n",
    "    if not isinstance(set2, set):\n",
    "        set2 = set(set2)\n",
    "    \n",
    "    intersection = len(set1.intersection(set2))\n",
    "    union = len(set1.union(set2))\n",
    "    \n",
    "    return intersection / union if union != 0 else 0\n",
    "\n",
    "## Average runtime - 1s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.1 ID Detection\n",
    "\n",
    "* Selecting unique words longer than five characters that are not included in English vocab of ParaCrawl dataset\n",
    "* Extracted 3 features: name_id, short_description_id, all_texts_id\n",
    "\n",
    "Paracrawl:\n",
    "* Data dimensions: 9 GB, 50,632,000 lines\n",
    "* Created english vocab with 4,400,347 unique tokens of length more than 5 \n",
    "* Used english vocab from NLTK to deal with resource shortage for computations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens in Paracrawl English vocab: 4400347\n"
     ]
    }
   ],
   "source": [
    "# Vocab from paracrawl dataset\n",
    "# Run paracrawl.py file separately to create list of tokens\n",
    "english_vocab = open(\"features/english_words.txt\", encoding='utf-8').read().splitlines()\n",
    "english_vocab = [word.lower() for word in english_vocab if len(word)>5]\n",
    "print(f\"Number of tokens in Paracrawl English vocab: {len(english_vocab)}\")\n",
    "\n",
    "## Average runtime - 2s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "29942088",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # NLTK english corpus\n",
    "# english_vocab = [word for word in words.words() if len(word)>5]\n",
    "\n",
    "def detect_unique_ids(text, vocab=english_vocab):\n",
    "    \"\"\"\n",
    "    Detect unique words longer than five characters not in the vocabulary\n",
    "    Args:\n",
    "        text (str): string containing product information\n",
    "        vocab (list): list of tokens\n",
    "    \n",
    "    Return:\n",
    "        list: words that are not part of english vocab\n",
    "    \"\"\"\n",
    "\n",
    "    words = re.findall(r'\\b[\\w\\-.,\\'!$&*]{6,}\\b', text.lower())\n",
    "    return [word for word in words if word not in vocab]\n",
    "\n",
    "def calculate_id_detection(text1, text2, vocab=english_vocab):\n",
    "    \"\"\"\n",
    "    Calculate ID detection keywords similarity between two products \n",
    "    Args:\n",
    "        text1 (str): string containing product information\n",
    "        text2 (str): string containing product information\n",
    "        vocab (list): list of tokens\n",
    "        \n",
    "    Return:\n",
    "        int: jaccard similarity between the IDs\n",
    "    \"\"\"\n",
    "    set1 = detect_unique_ids(text1, vocab)\n",
    "    set2 = detect_unique_ids(text2, vocab)\n",
    "    \n",
    "    return jaccard_sim(set1, set2)\n",
    "\n",
    "## Average runtime - 1s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "eb04f750",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate cosine similarity between name, short and long description columns of 2 products\n",
    "features_df['name_id'] = processed_df.apply(lambda row: calculate_id_detection(row['name1'], row['name2']), axis=1)\n",
    "features_df['short_description_id'] = processed_df.apply(lambda row: calculate_id_detection(row['short_description1'], row['short_description2']), axis=1)\n",
    "features_df['all_texts_id'] = processed_df.apply(lambda row: calculate_id_detection(row['all_texts1'], row['all_texts2']), axis=1)\n",
    "\n",
    "## Average runtime - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df['name_id']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b750998f",
   "metadata": {},
   "source": [
    "### 4.2.2 Brand Detections\n",
    "\n",
    "* Brand vocabulary created by processing the specification columns of source and target website\n",
    "* Extracted 3 features: name_brand, short_description_brand, all_texts_brand\n",
    "\n",
    "`REF` - https://stackoverflow.com/questions/5319922/check-if-a-word-is-in-a-string-in-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c0c4590a",
   "metadata": {},
   "outputs": [],
   "source": [
    "brand_vocab = text_processor.process_brand_names(\"specification1\")\n",
    "brand_vocab += text_processor.process_brand_names(\"specification2\")\n",
    "brand_vocab = set(brand_vocab)\n",
    "\n",
    "def findWholeWord(w):\n",
    "    return re.compile(r'\\b({0})\\b'.format(w), flags=re.IGNORECASE).search\n",
    "\n",
    "def detect_brands(text, brand_vocab=brand_vocab):\n",
    "    \"\"\"\n",
    "    Detect brands in the given text\n",
    "    Args:\n",
    "        text (str): string from the product information\n",
    "        brand_vocab (list): list of brands\n",
    "    \n",
    "    Returns:\n",
    "        list: brands detected in the product info\n",
    "    \"\"\"\n",
    "\n",
    "    return [word.lower() for word in brand_vocab if findWholeWord(word)(text)]\n",
    "\n",
    "\n",
    "def calculate_brand_detection(text1, text2, vocab=brand_vocab):\n",
    "    \"\"\"\n",
    "    Jaccard Similarity between the identified brands\n",
    "    Args:\n",
    "        text1 (str): string from the product information\n",
    "        text2 (str): string from the product information\n",
    "        brand_vocab (list): list of brands\n",
    "    \n",
    "    Returns:\n",
    "        int: Jaccard similarity between the brands of 2 products\n",
    "    \"\"\"\n",
    "    \n",
    "    set1 = detect_brands(text1, vocab)\n",
    "    set2 = detect_brands(text2, vocab)\n",
    "    \n",
    "    return jaccard_sim(set1, set2)\n",
    "\n",
    "## Average runtime - 1s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d2412207",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df['name_brand'] = processed_df.apply(lambda row: calculate_brand_detection(row['name1'], row['name2']), axis=1)\n",
    "features_df['short_description_brand'] = processed_df.apply(lambda row: calculate_brand_detection(row['short_description1'], row['short_description2']), axis=1)\n",
    "features_df['all_texts_brand'] = processed_df.apply(lambda row: calculate_brand_detection(row['all_texts1'], row['all_texts2']), axis=1)\n",
    "\n",
    "## Average runtime - 1m 30s\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd0025ef",
   "metadata": {},
   "source": [
    "### 4.2.3 Number Detections\n",
    "\n",
    "* If no units are found near the number, the number is detected as a free number\n",
    "* Free numbers can contain model numbers or other crucial information\n",
    "* Extracted 5 features: name_numbers, short_description_numbers, long_description_numbers, specification_text_numbers, all_texts_numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e8a5511e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_numbers(text):\n",
    "    \"\"\"\n",
    "    Detect free numbers in the given text\n",
    "    Args:\n",
    "        text (str): string from the product information\n",
    "    \n",
    "    Returns:\n",
    "        list: free numbers detected in the product info\n",
    "    \"\"\"\n",
    "    \n",
    "    return [float(match.group()) for match in re.finditer(r'\\b\\d+(\\.\\d+)?\\b', text)]\n",
    "\n",
    "def calculate_numbers_detection(text1, text2):\n",
    "    \"\"\"\n",
    "    Jaccard Similarity between the identified free numbers \n",
    "    Args:\n",
    "        text1 (str): string from the product information\n",
    "        text2 (str): string from the product information\n",
    "    \n",
    "    Returns:\n",
    "        int: Jaccard similarity between the free numbers of 2 products\n",
    "    \"\"\"\n",
    "    \n",
    "    set1 = detect_numbers(text1)\n",
    "    set2 = detect_numbers(text2)\n",
    "    \n",
    "    return jaccard_sim(set1, set2)\n",
    "\n",
    "## Average runtime - 1s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d6b7c314",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df['name_numbers'] = processed_df.apply(lambda row: calculate_numbers_detection(row['name1'], row['name2']), axis=1)\n",
    "features_df['short_description_numbers'] = processed_df.apply(lambda row: calculate_numbers_detection(row['short_description1'], row['short_description2']), axis=1)\n",
    "features_df['long_description_numbers'] = processed_df.apply(lambda row: calculate_numbers_detection(row['long_description1'], row['long_description2']), axis=1)\n",
    "features_df['specification_text_numbers'] = processed_df.apply(lambda row: calculate_numbers_detection(row['specification1'], row['specification2']), axis=1)\n",
    "features_df['all_texts_numbers'] = processed_df.apply(lambda row: calculate_numbers_detection(row['all_texts1'], row['all_texts2']), axis=1)\n",
    "\n",
    "## Average runtime - 1s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aefcdd56",
   "metadata": {},
   "source": [
    "### 4.2.4 Descriptive words\n",
    "\n",
    "* Set of the most characterising words for each attribute of the product\n",
    "* Extracted 4 features: name_descriptives, short_description_descriptives, long_description_descriptives, all_texts_descriptives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_word_document_frequency(word,documents):\n",
    "    counter = 0\n",
    "    for document in documents:\n",
    "        if word in document:\n",
    "            counter+=1\n",
    "    return counter\n",
    "\n",
    "def create_word_frequency_dict(dataframe, column):\n",
    "    words = dict()\n",
    "    for row in dataframe[column]:\n",
    "        for word in word_tokenize(row):\n",
    "            words[word] = words.get(word,calculate_word_document_frequency(word,dataframe[column]))\n",
    "    return words\n",
    "\n",
    "## Average runtime - 1s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "name1_words_frequency_dict = create_word_frequency_dict(processed_df,'name1')\n",
    "name2_words_frequency_dict = create_word_frequency_dict(processed_df,'name2')\n",
    "short_description1_words_frequency_dict = create_word_frequency_dict(processed_df,'short_description1')\n",
    "short_description2_words_frequency_dict = create_word_frequency_dict(processed_df,'short_description2')\n",
    "long_description1_words_frequency_dict = create_word_frequency_dict(processed_df,'long_description1')\n",
    "long_description2_words_frequency_dict = create_word_frequency_dict(processed_df,'long_description2')\n",
    "all_texts1_words_frequency_dict = create_word_frequency_dict(processed_df,'all_texts1')\n",
    "all_texts2_words_frequency_dict = create_word_frequency_dict(processed_df,'all_texts2')\n",
    "\n",
    "## Average runtime - 23min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "e7c0fd5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_descriptive_words(text,words_frequency_dict,documents_len,top_k = 50, maximum_p = 0.5):\n",
    "    \"\"\"\n",
    "    Detect descriptive words in the given text\n",
    "    Args:\n",
    "        text (str): string from the product information\n",
    "    \n",
    "    Returns:\n",
    "        list: descriptive words detected in the product info\n",
    "    \"\"\"\n",
    "    all_words =  [(word,words_frequency_dict[word]) for word in word_tokenize(text) if words_frequency_dict[word] < maximum_p*documents_len]\n",
    "    all_words.sort(key = lambda row: row[0])\n",
    "    return all_words[-top_k:]\n",
    "\n",
    "def calculate_descriptive_words(text1, text2,words_frequency_dict1,words_frequency_dict2,documents_len):\n",
    "    \"\"\"\n",
    "    Jaccard Similarity between the identified descriptive words \n",
    "    Args:\n",
    "        text1 (str): string from the product information\n",
    "        text2 (str): string from the product information\n",
    "    \n",
    "    Returns:\n",
    "        int: Jaccard similarity between the descriptive words of 2 products\n",
    "    \"\"\"\n",
    "    \n",
    "    set1 = detect_descriptive_words(text1,words_frequency_dict1,documents_len)\n",
    "    set2 = detect_descriptive_words(text2,words_frequency_dict2,documents_len)\n",
    "    \n",
    "    return jaccard_sim(set1, set2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents_len = len(processed_df)\n",
    "features_df['name_descriptives'] = processed_df.apply(lambda row: calculate_descriptive_words(row['name1'], row['name2'],name1_words_frequency_dict,name2_words_frequency_dict,documents_len), axis=1)\n",
    "\n",
    "features_df['short_description_descriptives'] = processed_df.apply(lambda row: calculate_descriptive_words(row['short_description1'], row['short_description2'],short_description1_words_frequency_dict,short_description2_words_frequency_dict,documents_len), axis=1)\n",
    "\n",
    "features_df['long_description_descriptives'] = processed_df.apply(lambda row: calculate_descriptive_words(row['long_description1'], row['long_description2'],long_description1_words_frequency_dict,long_description2_words_frequency_dict,documents_len), axis=1)\n",
    "\n",
    "features_df['all_texts_descriptives'] = processed_df.apply(lambda row: calculate_descriptive_words(row['all_texts1'], row['all_texts2'],all_texts1_words_frequency_dict,all_texts2_words_frequency_dict,documents_len), axis=1)\n",
    "\n",
    "## Average runtime - 4s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "712ffae5",
   "metadata": {},
   "source": [
    "### 4.2.5 Unit Detection\n",
    "\n",
    "* Extraction of numbers followed by units from each attribute \n",
    "* Extracted 5 features: name_units, short_description_units, long_description_units, specification_text_units, all_texts_units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "21364e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_units(text):\n",
    "    \"\"\"\n",
    "    Detect numbers which are accompanied by units\n",
    "    Args:\n",
    "        text (str): string from the product information\n",
    "    \n",
    "    Returns:\n",
    "        list: numbers around units detected in the product info\n",
    "    \"\"\"\n",
    "    matches = re.findall(r'\\b(\\d+(\\.\\d+)?)\\s*([a-zA-Z]+)\\b', text)\n",
    "\n",
    "    return [match[0] for match in matches]\n",
    "\n",
    "def calculate_unit_detection(text1, text2):\n",
    "    \"\"\"\n",
    "    Jaccard Similarity between the identified descriptive words \n",
    "    Args:\n",
    "        text1 (str): string from the product information\n",
    "        text2 (str): string from the product information\n",
    "    \n",
    "    Returns:\n",
    "        int: Jaccard similarity between the detected numbers around units of 2 products\n",
    "    \"\"\"\n",
    "    set1 = detect_units(text1)\n",
    "    set2 = detect_units(text2)\n",
    "    \n",
    "    return jaccard_sim(set1, set2)\n",
    "\n",
    "## Average runtime - 1s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e6d9a049",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df['name_units'] = processed_df.apply(lambda row: calculate_unit_detection(row['name1'], row['name2']), axis=1)\n",
    "features_df['short_description_units'] = processed_df.apply(lambda row: calculate_unit_detection(row['short_description1'], row['short_description2']), axis=1)\n",
    "features_df['long_description_units'] = processed_df.apply(lambda row: calculate_unit_detection(row['long_description1'], row['long_description2']), axis=1)\n",
    "features_df['specification_text_units'] = processed_df.apply(lambda row: calculate_unit_detection(row['specification1'], row['specification2']), axis=1)\n",
    "features_df['all_texts_units'] = processed_df.apply(lambda row: calculate_unit_detection(row['all_texts1'], row['all_texts2']), axis=1)\n",
    "\n",
    "## Average runtime - 1s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b6caa61",
   "metadata": {},
   "source": [
    "### 4.2.6 Words\n",
    "\n",
    "* Ratio of the same words taking all words from corresponding attributes of two products \n",
    "* Extracted 3 features: name_words, short_description_words, all_texts_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6210bbe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_words(text1, text2):\n",
    "    \"\"\"\n",
    "    Ratio of common words to all words \n",
    "    Args:\n",
    "        text1 (str): string from the product information\n",
    "        text2 (str): string from the product information\n",
    "    \n",
    "    Returns:\n",
    "        int: Jaccard similarity between the words of 2 products\n",
    "    \"\"\"\n",
    "    \n",
    "    set1 = set(word for word in text1.lower())\n",
    "    set2 = set(word for word in text2.lower())\n",
    "    \n",
    "    return jaccard_sim(set1, set2)\n",
    "\n",
    "## Average runtime - 1s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0e8d4fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df['name_words'] = processed_df.apply(lambda row: calculate_words(row['name1'], row['name2']), axis=1)\n",
    "features_df['short_description_words'] = processed_df.apply(lambda row: calculate_words(row['short_description1'], row['short_description2']), axis=1)\n",
    "features_df['all_texts_words'] = processed_df.apply(lambda row: calculate_words(row['all_texts1'], row['all_texts2']), axis=1)\n",
    "\n",
    "## Average runtime - 1s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d56be07",
   "metadata": {},
   "source": [
    "### 4.3 All Detected Keywords Comparisons\n",
    "\n",
    "* Ratio of matching values in those lists between two compared products\n",
    "* Extracted 4 features: all_units_list, all_ids_list, all_numbers_list, all_brands_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fb4f3172",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ratio(list1, list2):\n",
    "    \"\"\"\n",
    "    Ratio of common elements to total (can contain repetitions)\n",
    "    Args:\n",
    "        list1 (list): result from each type of detection\n",
    "        list2 (list): result from each type of detection\n",
    "    \n",
    "    Return:\n",
    "        int: common/total\n",
    "    \"\"\"\n",
    "    \n",
    "    total = len(list1 + list2)    \n",
    "    common = len([element for element in list1 if element in list2])\n",
    "    \n",
    "    return common/total if total != 0 else 0\n",
    "\n",
    "def calculate_all_units(text1, text2):\n",
    "    \"\"\"\n",
    "    Ratio of all units\n",
    "    Args:\n",
    "        text1 (str): string from the product information\n",
    "        text2 (str): string from the product information\n",
    "    \n",
    "    Returns:\n",
    "        int: identifiers between the 2 products\n",
    "    \"\"\"\n",
    "    \n",
    "    list1 = detect_units(text1)\n",
    "    list2 = detect_units(text2)\n",
    "    \n",
    "    return ratio(list1, list2)\n",
    "    \n",
    "def calculate_all_ids(text1, text2):\n",
    "    \"\"\"\n",
    "    Ratio of all units\n",
    "    Args:\n",
    "        text1 (str): string from the product information\n",
    "        text2 (str): string from the product information\n",
    "    \n",
    "    Returns:\n",
    "        int: identifiers between the 2 products\n",
    "    \"\"\"\n",
    "    \n",
    "    list1 = detect_unique_ids(text1)\n",
    "    list2 = detect_unique_ids(text2)\n",
    "    \n",
    "    return ratio(list1, list2)\n",
    "\n",
    "def calculate_all_numbers(text1, text2):\n",
    "    \"\"\"\n",
    "    Ratio of all units\n",
    "    Args:\n",
    "        text1 (str): string from the product information\n",
    "        text2 (str): string from the product information\n",
    "    \n",
    "    Returns:\n",
    "        int: identifiers between the 2 products\n",
    "    \"\"\"\n",
    "    \n",
    "    list1 = detect_numbers(text1)\n",
    "    list2 = detect_numbers(text2)\n",
    "    \n",
    "    return ratio(list1, list2)\n",
    "\n",
    "def calculate_all_brands(text1, text2):\n",
    "    \"\"\"\n",
    "    Ratio of all units\n",
    "    Args:\n",
    "        text1 (str): string from the product information\n",
    "        text2 (str): string from the product information\n",
    "    \n",
    "    Returns:\n",
    "        int: identifiers between the 2 products\n",
    "    \"\"\"\n",
    "    \n",
    "    list1 = detect_brands(text1)\n",
    "    list2 = detect_brands(text2)\n",
    "    \n",
    "    return ratio(list1, list2)\n",
    "## Average runtime - 1s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9ff88aef",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'detect_unique_ids' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32md:\\567Project\\Feature-Engineering-Part-2.ipynb Cell 41\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/567Project/Feature-Engineering-Part-2.ipynb#X55sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m features_df[\u001b[39m'\u001b[39m\u001b[39mall_units_list\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m processed_df\u001b[39m.\u001b[39mapply(\u001b[39mlambda\u001b[39;00m row: calculate_all_units(row[\u001b[39m'\u001b[39m\u001b[39mall_texts1\u001b[39m\u001b[39m'\u001b[39m], row[\u001b[39m'\u001b[39m\u001b[39mall_texts2\u001b[39m\u001b[39m'\u001b[39m]), axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/567Project/Feature-Engineering-Part-2.ipynb#X55sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m features_df[\u001b[39m'\u001b[39m\u001b[39mall_ids_list\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m processed_df\u001b[39m.\u001b[39;49mapply(\u001b[39mlambda\u001b[39;49;00m row: calculate_all_ids(row[\u001b[39m'\u001b[39;49m\u001b[39mall_texts1\u001b[39;49m\u001b[39m'\u001b[39;49m], row[\u001b[39m'\u001b[39;49m\u001b[39mall_texts2\u001b[39;49m\u001b[39m'\u001b[39;49m]), axis\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/567Project/Feature-Engineering-Part-2.ipynb#X55sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m features_df[\u001b[39m'\u001b[39m\u001b[39mall_numbers_list\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m processed_df\u001b[39m.\u001b[39mapply(\u001b[39mlambda\u001b[39;00m row: calculate_all_numbers(row[\u001b[39m'\u001b[39m\u001b[39mall_texts1\u001b[39m\u001b[39m'\u001b[39m], row[\u001b[39m'\u001b[39m\u001b[39mall_texts2\u001b[39m\u001b[39m'\u001b[39m]), axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/567Project/Feature-Engineering-Part-2.ipynb#X55sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m features_df[\u001b[39m'\u001b[39m\u001b[39mall_brands_list\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m processed_df\u001b[39m.\u001b[39mapply(\u001b[39mlambda\u001b[39;00m row: calculate_all_brands(row[\u001b[39m'\u001b[39m\u001b[39mall_texts1\u001b[39m\u001b[39m'\u001b[39m], row[\u001b[39m'\u001b[39m\u001b[39mall_texts2\u001b[39m\u001b[39m'\u001b[39m]), axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[1;32md:\\.virtualenvs\\567Project-7k1jFqDZ\\lib\\site-packages\\pandas\\core\\frame.py:10034\u001b[0m, in \u001b[0;36mDataFrame.apply\u001b[1;34m(self, func, axis, raw, result_type, args, by_row, **kwargs)\u001b[0m\n\u001b[0;32m  10022\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpandas\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mapply\u001b[39;00m \u001b[39mimport\u001b[39;00m frame_apply\n\u001b[0;32m  10024\u001b[0m op \u001b[39m=\u001b[39m frame_apply(\n\u001b[0;32m  10025\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m  10026\u001b[0m     func\u001b[39m=\u001b[39mfunc,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m  10032\u001b[0m     kwargs\u001b[39m=\u001b[39mkwargs,\n\u001b[0;32m  10033\u001b[0m )\n\u001b[1;32m> 10034\u001b[0m \u001b[39mreturn\u001b[39;00m op\u001b[39m.\u001b[39;49mapply()\u001b[39m.\u001b[39m__finalize__(\u001b[39mself\u001b[39m, method\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mapply\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32md:\\.virtualenvs\\567Project-7k1jFqDZ\\lib\\site-packages\\pandas\\core\\apply.py:837\u001b[0m, in \u001b[0;36mFrameApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    834\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mraw:\n\u001b[0;32m    835\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapply_raw()\n\u001b[1;32m--> 837\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply_standard()\n",
      "File \u001b[1;32md:\\.virtualenvs\\567Project-7k1jFqDZ\\lib\\site-packages\\pandas\\core\\apply.py:963\u001b[0m, in \u001b[0;36mFrameApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    962\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply_standard\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m--> 963\u001b[0m     results, res_index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply_series_generator()\n\u001b[0;32m    965\u001b[0m     \u001b[39m# wrap results\u001b[39;00m\n\u001b[0;32m    966\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwrap_results(results, res_index)\n",
      "File \u001b[1;32md:\\.virtualenvs\\567Project-7k1jFqDZ\\lib\\site-packages\\pandas\\core\\apply.py:979\u001b[0m, in \u001b[0;36mFrameApply.apply_series_generator\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    976\u001b[0m \u001b[39mwith\u001b[39;00m option_context(\u001b[39m\"\u001b[39m\u001b[39mmode.chained_assignment\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m):\n\u001b[0;32m    977\u001b[0m     \u001b[39mfor\u001b[39;00m i, v \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(series_gen):\n\u001b[0;32m    978\u001b[0m         \u001b[39m# ignore SettingWithCopy here in case the user mutates\u001b[39;00m\n\u001b[1;32m--> 979\u001b[0m         results[i] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfunc(v, \u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkwargs)\n\u001b[0;32m    980\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(results[i], ABCSeries):\n\u001b[0;32m    981\u001b[0m             \u001b[39m# If we have a view on v, we need to make a copy because\u001b[39;00m\n\u001b[0;32m    982\u001b[0m             \u001b[39m#  series_generator will swap out the underlying data\u001b[39;00m\n\u001b[0;32m    983\u001b[0m             results[i] \u001b[39m=\u001b[39m results[i]\u001b[39m.\u001b[39mcopy(deep\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "\u001b[1;32md:\\567Project\\Feature-Engineering-Part-2.ipynb Cell 41\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/567Project/Feature-Engineering-Part-2.ipynb#X55sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m features_df[\u001b[39m'\u001b[39m\u001b[39mall_units_list\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m processed_df\u001b[39m.\u001b[39mapply(\u001b[39mlambda\u001b[39;00m row: calculate_all_units(row[\u001b[39m'\u001b[39m\u001b[39mall_texts1\u001b[39m\u001b[39m'\u001b[39m], row[\u001b[39m'\u001b[39m\u001b[39mall_texts2\u001b[39m\u001b[39m'\u001b[39m]), axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/567Project/Feature-Engineering-Part-2.ipynb#X55sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m features_df[\u001b[39m'\u001b[39m\u001b[39mall_ids_list\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m processed_df\u001b[39m.\u001b[39mapply(\u001b[39mlambda\u001b[39;00m row: calculate_all_ids(row[\u001b[39m'\u001b[39;49m\u001b[39mall_texts1\u001b[39;49m\u001b[39m'\u001b[39;49m], row[\u001b[39m'\u001b[39;49m\u001b[39mall_texts2\u001b[39;49m\u001b[39m'\u001b[39;49m]), axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/567Project/Feature-Engineering-Part-2.ipynb#X55sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m features_df[\u001b[39m'\u001b[39m\u001b[39mall_numbers_list\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m processed_df\u001b[39m.\u001b[39mapply(\u001b[39mlambda\u001b[39;00m row: calculate_all_numbers(row[\u001b[39m'\u001b[39m\u001b[39mall_texts1\u001b[39m\u001b[39m'\u001b[39m], row[\u001b[39m'\u001b[39m\u001b[39mall_texts2\u001b[39m\u001b[39m'\u001b[39m]), axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/567Project/Feature-Engineering-Part-2.ipynb#X55sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m features_df[\u001b[39m'\u001b[39m\u001b[39mall_brands_list\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m processed_df\u001b[39m.\u001b[39mapply(\u001b[39mlambda\u001b[39;00m row: calculate_all_brands(row[\u001b[39m'\u001b[39m\u001b[39mall_texts1\u001b[39m\u001b[39m'\u001b[39m], row[\u001b[39m'\u001b[39m\u001b[39mall_texts2\u001b[39m\u001b[39m'\u001b[39m]), axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n",
      "\u001b[1;32md:\\567Project\\Feature-Engineering-Part-2.ipynb Cell 41\u001b[0m line \u001b[0;36m4\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/567Project/Feature-Engineering-Part-2.ipynb#X55sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcalculate_all_ids\u001b[39m(text1, text2):\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/567Project/Feature-Engineering-Part-2.ipynb#X55sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/567Project/Feature-Engineering-Part-2.ipynb#X55sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m \u001b[39m    Ratio of all units\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/567Project/Feature-Engineering-Part-2.ipynb#X55sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/567Project/Feature-Engineering-Part-2.ipynb#X55sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m \u001b[39m        int: identifiers between the 2 products\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/567Project/Feature-Engineering-Part-2.ipynb#X55sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/567Project/Feature-Engineering-Part-2.ipynb#X55sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m     list1 \u001b[39m=\u001b[39m detect_unique_ids(text1)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/567Project/Feature-Engineering-Part-2.ipynb#X55sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m     list2 \u001b[39m=\u001b[39m detect_unique_ids(text2)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/567Project/Feature-Engineering-Part-2.ipynb#X55sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m ratio(list1, list2)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'detect_unique_ids' is not defined"
     ]
    }
   ],
   "source": [
    "features_df['all_units_list'] = processed_df.apply(lambda row: calculate_all_units(row['all_texts1'], row['all_texts2']), axis=1)\n",
    "features_df['all_ids_list'] = processed_df.apply(lambda row: calculate_all_ids(row['all_texts1'], row['all_texts2']), axis=1)\n",
    "features_df['all_numbers_list'] = processed_df.apply(lambda row: calculate_all_numbers(row['all_texts1'], row['all_texts2']), axis=1)\n",
    "features_df['all_brands_list'] = processed_df.apply(lambda row: calculate_all_brands(row['all_texts1'], row['all_texts2']), axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "268d6f68",
   "metadata": {},
   "source": [
    "### 4.4 Specification preprocessing\n",
    "\n",
    "* Ratio of corresponding parameter names as specification_key\n",
    "* Ratio of corresponding parameter names and values as specification_key_value\n",
    "* Extracted 2 features: specification_key, specification_key_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5e576f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_key_value_match(text1, text2):\n",
    "    \"\"\"\n",
    "    Common key value pairs in the specification column\n",
    "    Args:\n",
    "        text1 (str): string from the product information\n",
    "        text2 (str): string from the product information\n",
    "    \n",
    "    Returns:\n",
    "        int: Ratio between common pairs and total pairs of the 2 products\n",
    "    \"\"\"\n",
    "    return ratio(eval(text1), eval(text2))\n",
    "\n",
    "def calculate_key_match(list1, list2):\n",
    "    \"\"\"\n",
    "    Common keys in the specification column\n",
    "    Args:\n",
    "        text1 (str): string from the product information\n",
    "        text2 (str): string from the product information\n",
    "    \n",
    "    Returns:\n",
    "        int: Ratio between common keys and total keys of the 2 products\n",
    "    \"\"\"\n",
    "    set1 = [d[\"key\"] for d in eval(list1)]\n",
    "    set2 = [d[\"key\"] for d in eval(list2)]\n",
    "\n",
    "    return jaccard_sim(set1, set2)\n",
    "\n",
    "## Average runtime - 1s\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7adc3ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df['specification_key'] = processed_df.apply(lambda row: calculate_key_match(row['orig_specification1'], row['orig_specification2']), axis=1)\n",
    "features_df['specification_key_value'] = processed_df.apply(lambda row: calculate_key_value_match(row['orig_specification1'], row['orig_specification2']), axis=1)\n",
    "\n",
    "## Average runtime - 1s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37340ee7",
   "metadata": {},
   "source": [
    "### 4.5 Add Image Hash Similarities\n",
    "\n",
    "* Join the image hash similarity with text processing features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a5592804",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1555, 1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hash_similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   hash_similarity\n",
       "0              0.0\n",
       "1              0.0\n",
       "2              0.0"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_hash_train = pd.read_csv(\"features/ProMapEn/images_train_similarties.csv\")\n",
    "image_hash_test = pd.read_csv(\"features/ProMapEn/images_test_similarties.csv\")\n",
    "\n",
    "image_hashes = pd.concat([image_hash_train, image_hash_test], ignore_index=True)\n",
    "\n",
    "print(image_hashes.shape)\n",
    "image_hashes.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "814c4a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df[\"hash_similarity\"] = image_hashes[\"hash_similarity\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d59aff",
   "metadata": {},
   "source": [
    "### 4.6 Label Stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e36cecb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "match\n",
       "0    1046\n",
       "1     509\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_df['match'] = processed_df[\"match\"]\n",
    "features_df['match'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b1b7ef15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1555, 29)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name_cos</th>\n",
       "      <th>short_description_cos</th>\n",
       "      <th>long_description_cos</th>\n",
       "      <th>all_texts_cos</th>\n",
       "      <th>name_words</th>\n",
       "      <th>short_description_words</th>\n",
       "      <th>all_texts_words</th>\n",
       "      <th>name_descriptives</th>\n",
       "      <th>short_description_descriptives</th>\n",
       "      <th>long_description_descriptives</th>\n",
       "      <th>...</th>\n",
       "      <th>name_units</th>\n",
       "      <th>short_description_units</th>\n",
       "      <th>long_description_units</th>\n",
       "      <th>specification_text_units</th>\n",
       "      <th>all_texts_units</th>\n",
       "      <th>all_units_list</th>\n",
       "      <th>specification_key</th>\n",
       "      <th>specification_key_value</th>\n",
       "      <th>hash_similarity</th>\n",
       "      <th>match</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.230623</td>\n",
       "      <td>0.096354</td>\n",
       "      <td>0.194154</td>\n",
       "      <td>0.567405</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.645161</td>\n",
       "      <td>0.853659</td>\n",
       "      <td>0.029412</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>...</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.214286</td>\n",
       "      <td>0.148148</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.178245</td>\n",
       "      <td>0.299044</td>\n",
       "      <td>0.377834</td>\n",
       "      <td>0.717010</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.735294</td>\n",
       "      <td>0.708333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.011905</td>\n",
       "      <td>...</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.277778</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.079730</td>\n",
       "      <td>0.549188</td>\n",
       "      <td>0.747834</td>\n",
       "      <td>0.787388</td>\n",
       "      <td>0.580645</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.680851</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.012048</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.706158</td>\n",
       "      <td>0.385939</td>\n",
       "      <td>0.272149</td>\n",
       "      <td>0.715220</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.617647</td>\n",
       "      <td>0.882353</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.036145</td>\n",
       "      <td>...</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.912291</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.346680</td>\n",
       "      <td>0.919648</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.703704</td>\n",
       "      <td>0.945946</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.012346</td>\n",
       "      <td>...</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   name_cos  short_description_cos  long_description_cos  all_texts_cos  \\\n",
       "0  0.230623               0.096354              0.194154       0.567405   \n",
       "1  0.178245               0.299044              0.377834       0.717010   \n",
       "2  0.079730               0.549188              0.747834       0.787388   \n",
       "3  0.706158               0.385939              0.272149       0.715220   \n",
       "4  0.912291               0.000000              0.346680       0.919648   \n",
       "\n",
       "   name_words  short_description_words  all_texts_words  name_descriptives  \\\n",
       "0    0.700000                 0.645161         0.853659           0.029412   \n",
       "1    0.700000                 0.735294         0.708333           0.000000   \n",
       "2    0.580645                 0.866667         0.680851           0.000000   \n",
       "3    0.916667                 0.617647         0.882353           0.136364   \n",
       "4    1.000000                 0.703704         0.945946           0.166667   \n",
       "\n",
       "   short_description_descriptives  long_description_descriptives  ...  \\\n",
       "0                             0.0                       0.016949  ...   \n",
       "1                             0.0                       0.011905  ...   \n",
       "2                             0.0                       0.012048  ...   \n",
       "3                             0.0                       0.036145  ...   \n",
       "4                             0.0                       0.012346  ...   \n",
       "\n",
       "   name_units  short_description_units  long_description_units  \\\n",
       "0    0.166667                    0.000                   0.000   \n",
       "1    0.500000                    0.500                   0.250   \n",
       "2    0.000000                    0.000                   0.000   \n",
       "3    0.500000                    0.125                   0.000   \n",
       "4    0.500000                    0.000                   0.125   \n",
       "\n",
       "   specification_text_units  all_texts_units  all_units_list  \\\n",
       "0                       0.0         0.214286        0.148148   \n",
       "1                       0.0         0.333333        0.277778   \n",
       "2                       0.0         0.090909        0.083333   \n",
       "3                       0.0         0.363636        0.285714   \n",
       "4                       0.0         0.714286        0.466667   \n",
       "\n",
       "   specification_key  specification_key_value  hash_similarity  match  \n",
       "0                0.0                      0.0              0.0      0  \n",
       "1                0.0                      0.0              0.0      0  \n",
       "2                0.0                      0.0              0.0      0  \n",
       "3                0.0                      0.0              0.0      1  \n",
       "4                0.0                      0.0              0.0      0  \n",
       "\n",
       "[5 rows x 29 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(features_df.shape)\n",
    "features_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dbaa47e",
   "metadata": {},
   "source": [
    "## 5.0 Save the features into CSV file\n",
    "\n",
    "* Train set: 1244 rows\n",
    "* Test set: 311 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a9687531",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the training data (1244, 29)\n",
      "Shape of the testing data (311, 29)\n"
     ]
    }
   ],
   "source": [
    "train_df = features_df.iloc[:1244, :]\n",
    "test_df = features_df.iloc[1244:, :]\n",
    "\n",
    "print(\"Shape of the training data\", train_df.shape)\n",
    "print(\"Shape of the testing data\", test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "819ff32e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.to_csv(\"features/ProMapEn/promapen_train_similarities.csv\", header=True, index=False)\n",
    "test_df.to_csv(\"features/ProMapEn/promapen_test_similarities.csv\", header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Reading the features again\n",
    "\n",
    "# train_df = pd.read_csv(\"features/ProMapEn/promapen_train_similarities.csv\")\n",
    "# test_df = pd.read_csv(\"features/ProMapEn/promapen_test_similarities.csv\")\n",
    "# features_df = pd.concat([train_df, test_df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'all_brands_list',\n",
       " 'all_ids_list',\n",
       " 'all_numbers_list',\n",
       " 'all_texts_id',\n",
       " 'name_id',\n",
       " 'short_description_id'}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columnslist = ['name_cos','long_description_descriptives','all_texts_cos','all_numbers_list','name_id','long_description_units','all_texts_brand','all_ids_list','name_brand','short_description_cos','all_texts_id','all_units_list','name_numbers','short_description_id','all_texts_numbers','specification_text_numbers','name_descriptives','short_description_brand','all_texts_descriptives','specification_text_units','name_units','short_description_numbers','all_texts_units','specification_key','name_words','short_description_descriptives','all_texts_words','specification_key_value','long_description_cos','short_description_units','all_brands_list','hash_similarity','long_description_numbers','short_description_words']\n",
    "\n",
    "set(columnslist) - set(features_df.columns)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
