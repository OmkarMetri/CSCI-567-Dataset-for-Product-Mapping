{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9726bcee",
   "metadata": {},
   "source": [
    "# Feaure Engineering - Text Processing\n",
    "\n",
    "* Process the textual data\n",
    "* Combine the hash similarities of the images (Feature-Engineering-Part-1)\n",
    "* Save the features to CSV files in the \"features/ProMapEn/\" path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce422480",
   "metadata": {},
   "source": [
    "## 1. Modules and Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ccdc76b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\shubh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\shubh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import contractions\n",
    "\n",
    "import nltk\n",
    "nltk.download('words')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import words\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "## Average runtime - 1s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec28a315",
   "metadata": {},
   "source": [
    "## 2. Load data\n",
    "\n",
    "* Combined the train and test for feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6331bba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1555, 21)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name1</th>\n",
       "      <th>short_description1</th>\n",
       "      <th>long_description1</th>\n",
       "      <th>specification1</th>\n",
       "      <th>image1</th>\n",
       "      <th>price1</th>\n",
       "      <th>id1</th>\n",
       "      <th>name2</th>\n",
       "      <th>short_description2</th>\n",
       "      <th>long_description2</th>\n",
       "      <th>...</th>\n",
       "      <th>image2</th>\n",
       "      <th>price2</th>\n",
       "      <th>id2</th>\n",
       "      <th>match</th>\n",
       "      <th>image_url1</th>\n",
       "      <th>image_url2</th>\n",
       "      <th>category</th>\n",
       "      <th>match_type</th>\n",
       "      <th>specification_text1</th>\n",
       "      <th>specification_text2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bagcraft P057012 12 x 12 Grease-Resistant Pape...</td>\n",
       "      <td>Excellent low-cost, low-waste alternative to p...</td>\n",
       "      <td>Wrap/liner is an excellent low-cost, low-waste...</td>\n",
       "      <td>[{\"key\": \"Features\", \"value\": \"Excellent low-c...</td>\n",
       "      <td>3</td>\n",
       "      <td>131.59</td>\n",
       "      <td>https://walmart.com/ip/Bagcraft-P057012-12-x-1...</td>\n",
       "      <td>Bagcraft Papercon 012008 Interfolded Heavy Dry...</td>\n",
       "      <td>Provides wet strength, improved moisture resis...</td>\n",
       "      <td>Bagcraft interfolded heavy dry wax deli paper....</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>135.1</td>\n",
       "      <td>https://www.amazon.com/dp/B00C7KTHHI</td>\n",
       "      <td>0</td>\n",
       "      <td>[\"https://i5.walmartimages.com/asr/8f9b23a7-f4...</td>\n",
       "      <td>[\"https://m.media-amazon.com/images/I/51VDhs3N...</td>\n",
       "      <td>6_household</td>\n",
       "      <td>medium_nonmatch</td>\n",
       "      <td>Features Excellent low-cost, low-waste alterna...</td>\n",
       "      <td>Brand Name Bagcraft Papercon Global Trade Iden...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Clorox 35420 128 oz. Clean-Up Disinfectant Cle...</td>\n",
       "      <td>Removes stains and disinfects to kill 99.9% of...</td>\n",
       "      <td>Clorox Clean-Up CloroxPro Disinfectant Cleaner...</td>\n",
       "      <td>[{\"key\": \"Assembled Product Weight\", \"value\": ...</td>\n",
       "      <td>5</td>\n",
       "      <td>61.38</td>\n",
       "      <td>https://walmart.com/ip/Clorox-35420-128-oz-Cle...</td>\n",
       "      <td>CloroxPro Anywhere Daily Disinfectant and Sani...</td>\n",
       "      <td>NO-RINSE FOOD CONTACT SANITIZER: Confidently s...</td>\n",
       "      <td>CloroxPro Anywhere Daily Disinfectant and Sani...</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>https://www.amazon.com/dp/B07FQRB2XV</td>\n",
       "      <td>0</td>\n",
       "      <td>[\"https://i5.walmartimages.com/asr/3336afe6-d5...</td>\n",
       "      <td>[\"https://m.media-amazon.com/images/I/71f6nNyY...</td>\n",
       "      <td>6_household</td>\n",
       "      <td>close_nonmatch</td>\n",
       "      <td>Assembled Product Weight 37.4 lb Brand Clorox ...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Clorox 35420 128 oz. Clean-Up Disinfectant Cle...</td>\n",
       "      <td>Removes stains and disinfects to kill 99.9% of...</td>\n",
       "      <td>Clorox Clean-Up CloroxPro Disinfectant Cleaner...</td>\n",
       "      <td>[{\"key\": \"Assembled Product Weight\", \"value\": ...</td>\n",
       "      <td>5</td>\n",
       "      <td>61.38</td>\n",
       "      <td>https://walmart.com/ip/Clorox-35420-128-oz-Cle...</td>\n",
       "      <td>CLOROXPRO Commercial Solutions CLOROXPRO Clean...</td>\n",
       "      <td>DISINFECTANT SPRAY: Use this Clorox Clean-Up D...</td>\n",
       "      <td>Clorox Clean-Up Disinfectant Cleaner with Blea...</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>83.6</td>\n",
       "      <td>https://www.amazon.com/dp/B004EHZ7GW</td>\n",
       "      <td>0</td>\n",
       "      <td>[\"https://i5.walmartimages.com/asr/3336afe6-d5...</td>\n",
       "      <td>[\"https://m.media-amazon.com/images/I/81+djgUF...</td>\n",
       "      <td>6_household</td>\n",
       "      <td>medium_nonmatch</td>\n",
       "      <td>Assembled Product Weight 37.4 lb Brand Clorox ...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows Ã— 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               name1  \\\n",
       "0  Bagcraft P057012 12 x 12 Grease-Resistant Pape...   \n",
       "1  Clorox 35420 128 oz. Clean-Up Disinfectant Cle...   \n",
       "2  Clorox 35420 128 oz. Clean-Up Disinfectant Cle...   \n",
       "\n",
       "                                  short_description1  \\\n",
       "0  Excellent low-cost, low-waste alternative to p...   \n",
       "1  Removes stains and disinfects to kill 99.9% of...   \n",
       "2  Removes stains and disinfects to kill 99.9% of...   \n",
       "\n",
       "                                   long_description1  \\\n",
       "0  Wrap/liner is an excellent low-cost, low-waste...   \n",
       "1  Clorox Clean-Up CloroxPro Disinfectant Cleaner...   \n",
       "2  Clorox Clean-Up CloroxPro Disinfectant Cleaner...   \n",
       "\n",
       "                                      specification1  image1  price1  \\\n",
       "0  [{\"key\": \"Features\", \"value\": \"Excellent low-c...       3  131.59   \n",
       "1  [{\"key\": \"Assembled Product Weight\", \"value\": ...       5   61.38   \n",
       "2  [{\"key\": \"Assembled Product Weight\", \"value\": ...       5   61.38   \n",
       "\n",
       "                                                 id1  \\\n",
       "0  https://walmart.com/ip/Bagcraft-P057012-12-x-1...   \n",
       "1  https://walmart.com/ip/Clorox-35420-128-oz-Cle...   \n",
       "2  https://walmart.com/ip/Clorox-35420-128-oz-Cle...   \n",
       "\n",
       "                                               name2  \\\n",
       "0  Bagcraft Papercon 012008 Interfolded Heavy Dry...   \n",
       "1  CloroxPro Anywhere Daily Disinfectant and Sani...   \n",
       "2  CLOROXPRO Commercial Solutions CLOROXPRO Clean...   \n",
       "\n",
       "                                  short_description2  \\\n",
       "0  Provides wet strength, improved moisture resis...   \n",
       "1  NO-RINSE FOOD CONTACT SANITIZER: Confidently s...   \n",
       "2  DISINFECTANT SPRAY: Use this Clorox Clean-Up D...   \n",
       "\n",
       "                                   long_description2  ... image2  price2  \\\n",
       "0  Bagcraft interfolded heavy dry wax deli paper....  ...      1   135.1   \n",
       "1  CloroxPro Anywhere Daily Disinfectant and Sani...  ...      1           \n",
       "2  Clorox Clean-Up Disinfectant Cleaner with Blea...  ...      1    83.6   \n",
       "\n",
       "                                    id2 match  \\\n",
       "0  https://www.amazon.com/dp/B00C7KTHHI     0   \n",
       "1  https://www.amazon.com/dp/B07FQRB2XV     0   \n",
       "2  https://www.amazon.com/dp/B004EHZ7GW     0   \n",
       "\n",
       "                                          image_url1  \\\n",
       "0  [\"https://i5.walmartimages.com/asr/8f9b23a7-f4...   \n",
       "1  [\"https://i5.walmartimages.com/asr/3336afe6-d5...   \n",
       "2  [\"https://i5.walmartimages.com/asr/3336afe6-d5...   \n",
       "\n",
       "                                          image_url2     category  \\\n",
       "0  [\"https://m.media-amazon.com/images/I/51VDhs3N...  6_household   \n",
       "1  [\"https://m.media-amazon.com/images/I/71f6nNyY...  6_household   \n",
       "2  [\"https://m.media-amazon.com/images/I/81+djgUF...  6_household   \n",
       "\n",
       "        match_type                                specification_text1  \\\n",
       "0  medium_nonmatch  Features Excellent low-cost, low-waste alterna...   \n",
       "1   close_nonmatch  Assembled Product Weight 37.4 lb Brand Clorox ...   \n",
       "2  medium_nonmatch  Assembled Product Weight 37.4 lb Brand Clorox ...   \n",
       "\n",
       "                                 specification_text2  \n",
       "0  Brand Name Bagcraft Papercon Global Trade Iden...  \n",
       "1                                                     \n",
       "2                                                     \n",
       "\n",
       "[3 rows x 21 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "promapen_train = pd.read_csv(\"datasets\\\\ProMapEn\\\\promapen-train_data.csv\")\n",
    "promapen_test = pd.read_csv(\"datasets\\\\ProMapEn\\\\promapen-test_data.csv\")\n",
    "\n",
    "promapen = pd.concat([promapen_train, promapen_test], ignore_index=True)\n",
    "promapen = promapen.fillna(\" \")\n",
    "\n",
    "print(promapen.shape)\n",
    "promapen.head(3)\n",
    "\n",
    "## Average runtime - 1s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caecde55",
   "metadata": {},
   "source": [
    "## 3. Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2588e7b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocess:\n",
    "    \"\"\"Preprocess the text\"\"\"\n",
    "    \n",
    "    def __init__(self, dataframe=None):\n",
    "        \"\"\"\n",
    "        Preprocess the data to extract features\n",
    "        Args:\n",
    "            dataframe (pd.Dataframe): data to be processed \n",
    "        \"\"\"\n",
    "        self.dataframe = dataframe\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        self.stopwords = stopwords.words('english')\n",
    "    \n",
    "    def process_brand_names(self, column):\n",
    "        \"\"\"\n",
    "        Extract the brand names from the specified column\n",
    "        Args:\n",
    "            column (str): name of the column\n",
    "        \n",
    "        Return:\n",
    "            list: brand names\n",
    "        \"\"\"\n",
    "        return [\n",
    "            item[\"value\"].lower()\n",
    "            for row in self.dataframe[column]\n",
    "            for item in eval(row)\n",
    "            if item[\"key\"]==\"Brand\" \n",
    "        ]\n",
    "    \n",
    "    def process_text_column(self, column):\n",
    "        \"\"\"\n",
    "        Process the text from the specified column\n",
    "        Args:\n",
    "            column (str): name of the column\n",
    "        \n",
    "        Return:\n",
    "            list: cleaned data\n",
    "        \"\"\"\n",
    "        \n",
    "        processed_values = []\n",
    "        \n",
    "        for text in self.dataframe[column]:\n",
    "            text = contractions.fix(text).lower() \n",
    "            text = re.sub(r\"[^\\w\\s]\", \" \", text) # remove useless characters\n",
    "            text = re.sub(r\"(\\d)([A-Za-z])\", r\"\\1 \\2\", text) # separate units and values\n",
    "\n",
    "            words = [word.strip() for word in word_tokenize(text)] \n",
    "            words = [word for word in words if word not in self.stopwords] # remove stopwords\n",
    "            words = [self.lemmatizer.lemmatize(word.lower()) for word in words] # lemmatize\n",
    "            \n",
    "            processed_values.append(\" \".join(words))\n",
    "        \n",
    "        return processed_values\n",
    "\n",
    "    def process_specification(self, column):\n",
    "        \"\"\"\n",
    "        Process the specification column of the products\n",
    "        Args:\n",
    "            column (str): name of the column\n",
    "            \n",
    "        Return:\n",
    "            list: string format after evaluating the row\n",
    "        \"\"\"\n",
    "        return [\n",
    "            ' '.join([f\"{item['key']} {item['value']}\" for item in eval(row)])\n",
    "            for row in self.dataframe[column]\n",
    "        ]\n",
    "\n",
    "## Average runtime - 1s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ec15eda",
   "metadata": {},
   "source": [
    "### 3.1 Process the dataframe\n",
    "\n",
    "* Store only the necessary columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c1b00ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_processor = Preprocess(promapen)\n",
    "processed_df = pd.DataFrame()\n",
    "\n",
    "# process name, short description and long description\n",
    "processed_df[\"name1\"] = text_processor.process_text_column(\"name1\")\n",
    "processed_df[\"short_description1\"] = text_processor.process_text_column(\"short_description1\")\n",
    "processed_df[\"long_description1\"] = text_processor.process_text_column(\"long_description1\")\n",
    "\n",
    "processed_df[\"name2\"] = text_processor.process_text_column(\"name2\")\n",
    "processed_df[\"short_description2\"] = text_processor.process_text_column(\"short_description2\")\n",
    "processed_df[\"long_description2\"] = text_processor.process_text_column(\"long_description2\")\n",
    "\n",
    "# process the specifications column\n",
    "processed_df[\"specification1\"] = text_processor.process_specification(\"specification1\")\n",
    "processed_df[\"specification2\"] = text_processor.process_specification(\"specification2\")\n",
    "\n",
    "# add all_texts column by combining values from name, short description and long description\n",
    "processed_df[\"all_texts1\"] = processed_df.apply(lambda row: \" \".join([row['name1'], row['short_description1'], row[\"long_description2\"], row[\"specification1\"]]), axis=1)\n",
    "processed_df[\"all_texts2\"] = processed_df.apply(lambda row: \" \".join([row['name2'], row['short_description2'], row[\"long_description2\"], row[\"specification2\"]]), axis=1)\n",
    "\n",
    "# add original specifications\n",
    "processed_df[\"orig_specification1\"] = promapen[\"specification1\"]\n",
    "processed_df[\"orig_specification2\"] = promapen[\"specification2\"]\n",
    "\n",
    "# attach the match_type column\n",
    "processed_df[\"match\"] = promapen[\"match\"]\n",
    "\n",
    "## Average runtime - 8s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "27d97355",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1555, 13)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name1</th>\n",
       "      <th>short_description1</th>\n",
       "      <th>long_description1</th>\n",
       "      <th>name2</th>\n",
       "      <th>short_description2</th>\n",
       "      <th>long_description2</th>\n",
       "      <th>specification1</th>\n",
       "      <th>specification2</th>\n",
       "      <th>all_texts1</th>\n",
       "      <th>all_texts2</th>\n",
       "      <th>orig_specification1</th>\n",
       "      <th>orig_specification2</th>\n",
       "      <th>match</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bagcraft p057012 12 x 12 grease resistant pape...</td>\n",
       "      <td>excellent low cost low waste alternative paper...</td>\n",
       "      <td>wrap liner excellent low cost low waste altern...</td>\n",
       "      <td>bagcraft papercon 012008 interfolded heavy dry...</td>\n",
       "      <td>provides wet strength improved moisture resist...</td>\n",
       "      <td>bagcraft interfolded heavy dry wax deli paper ...</td>\n",
       "      <td>Features Excellent low-cost, low-waste alterna...</td>\n",
       "      <td>Brand Name Bagcraft Papercon Global Trade Iden...</td>\n",
       "      <td>bagcraft p057012 12 x 12 grease resistant pape...</td>\n",
       "      <td>bagcraft papercon 012008 interfolded heavy dry...</td>\n",
       "      <td>[{\"key\": \"Features\", \"value\": \"Excellent low-c...</td>\n",
       "      <td>[{\"key\": \"Brand Name\", \"value\": \"Bagcraft Pape...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>clorox 35420 128 oz clean disinfectant cleaner...</td>\n",
       "      <td>remove stain disinfects kill 99 9 virus bacter...</td>\n",
       "      <td>clorox clean cloroxpro disinfectant cleaner bl...</td>\n",
       "      <td>cloroxpro anywhere daily disinfectant sanitizi...</td>\n",
       "      <td>rinse food contact sanitizer confidently sanit...</td>\n",
       "      <td>cloroxpro anywhere daily disinfectant sanitizi...</td>\n",
       "      <td>Assembled Product Weight 37.4 lb Brand Clorox ...</td>\n",
       "      <td></td>\n",
       "      <td>clorox 35420 128 oz clean disinfectant cleaner...</td>\n",
       "      <td>cloroxpro anywhere daily disinfectant sanitizi...</td>\n",
       "      <td>[{\"key\": \"Assembled Product Weight\", \"value\": ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>clorox 35420 128 oz clean disinfectant cleaner...</td>\n",
       "      <td>remove stain disinfects kill 99 9 virus bacter...</td>\n",
       "      <td>clorox clean cloroxpro disinfectant cleaner bl...</td>\n",
       "      <td>cloroxpro commercial solution cloroxpro clean ...</td>\n",
       "      <td>disinfectant spray use clorox clean disinfecta...</td>\n",
       "      <td>clorox clean disinfectant cleaner bleach power...</td>\n",
       "      <td>Assembled Product Weight 37.4 lb Brand Clorox ...</td>\n",
       "      <td></td>\n",
       "      <td>clorox 35420 128 oz clean disinfectant cleaner...</td>\n",
       "      <td>cloroxpro commercial solution cloroxpro clean ...</td>\n",
       "      <td>[{\"key\": \"Assembled Product Weight\", \"value\": ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               name1  \\\n",
       "0  bagcraft p057012 12 x 12 grease resistant pape...   \n",
       "1  clorox 35420 128 oz clean disinfectant cleaner...   \n",
       "2  clorox 35420 128 oz clean disinfectant cleaner...   \n",
       "\n",
       "                                  short_description1  \\\n",
       "0  excellent low cost low waste alternative paper...   \n",
       "1  remove stain disinfects kill 99 9 virus bacter...   \n",
       "2  remove stain disinfects kill 99 9 virus bacter...   \n",
       "\n",
       "                                   long_description1  \\\n",
       "0  wrap liner excellent low cost low waste altern...   \n",
       "1  clorox clean cloroxpro disinfectant cleaner bl...   \n",
       "2  clorox clean cloroxpro disinfectant cleaner bl...   \n",
       "\n",
       "                                               name2  \\\n",
       "0  bagcraft papercon 012008 interfolded heavy dry...   \n",
       "1  cloroxpro anywhere daily disinfectant sanitizi...   \n",
       "2  cloroxpro commercial solution cloroxpro clean ...   \n",
       "\n",
       "                                  short_description2  \\\n",
       "0  provides wet strength improved moisture resist...   \n",
       "1  rinse food contact sanitizer confidently sanit...   \n",
       "2  disinfectant spray use clorox clean disinfecta...   \n",
       "\n",
       "                                   long_description2  \\\n",
       "0  bagcraft interfolded heavy dry wax deli paper ...   \n",
       "1  cloroxpro anywhere daily disinfectant sanitizi...   \n",
       "2  clorox clean disinfectant cleaner bleach power...   \n",
       "\n",
       "                                      specification1  \\\n",
       "0  Features Excellent low-cost, low-waste alterna...   \n",
       "1  Assembled Product Weight 37.4 lb Brand Clorox ...   \n",
       "2  Assembled Product Weight 37.4 lb Brand Clorox ...   \n",
       "\n",
       "                                      specification2  \\\n",
       "0  Brand Name Bagcraft Papercon Global Trade Iden...   \n",
       "1                                                      \n",
       "2                                                      \n",
       "\n",
       "                                          all_texts1  \\\n",
       "0  bagcraft p057012 12 x 12 grease resistant pape...   \n",
       "1  clorox 35420 128 oz clean disinfectant cleaner...   \n",
       "2  clorox 35420 128 oz clean disinfectant cleaner...   \n",
       "\n",
       "                                          all_texts2  \\\n",
       "0  bagcraft papercon 012008 interfolded heavy dry...   \n",
       "1  cloroxpro anywhere daily disinfectant sanitizi...   \n",
       "2  cloroxpro commercial solution cloroxpro clean ...   \n",
       "\n",
       "                                 orig_specification1  \\\n",
       "0  [{\"key\": \"Features\", \"value\": \"Excellent low-c...   \n",
       "1  [{\"key\": \"Assembled Product Weight\", \"value\": ...   \n",
       "2  [{\"key\": \"Assembled Product Weight\", \"value\": ...   \n",
       "\n",
       "                                 orig_specification2  match  \n",
       "0  [{\"key\": \"Brand Name\", \"value\": \"Bagcraft Pape...      0  \n",
       "1                                                 []      0  \n",
       "2                                                 []      0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(processed_df.shape)\n",
    "processed_df.head(3)\n",
    "\n",
    "## Average runtime - 1s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_df.to_parquet('Processed.parquet')\n",
    "## Read when required\n",
    "# processed_df = pd.read_parquet('Processed.parquet')\n",
    "\n",
    "## Average runtime - 1s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8579a1f0",
   "metadata": {},
   "source": [
    "## 4. Feature Extraction\n",
    "\n",
    "### 4.1 Text similarity computations\n",
    "\n",
    "* Calculate cosine similarity between the textual information of 2 products\n",
    "* Extracted 4 features: name_cos, short_description_cos, long_description_cos, all_texts_cos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TFIDF_vectorizer:\n",
    "    \n",
    "    \n",
    "    def __init__(self,dataframe_column) -> None:\n",
    "        \"\"\"\n",
    "        Fitting the columns into TfidfVectorizer\n",
    "        Args:\n",
    "            series (pd.Series): data to be fitted \n",
    "        \"\"\"\n",
    "        self.vectorizer = TfidfVectorizer().fit(dataframe_column)\n",
    "    \n",
    "    def transformer(self, text1, text2) -> list:\n",
    "        \"\"\"\n",
    "        Transform the text into tfidf vectors\n",
    "        Args:\n",
    "            text1 (str): column 1 data\n",
    "            text2 (str): column 2 data\n",
    "        \"\"\"\n",
    "        return [self.vectorizer.transform([text1]),self.vectorizer.transform([text2])]\n",
    "    \n",
    "    def calculate_cosine_similarity(self, text1, text2):\n",
    "        \"\"\"\n",
    "        Calculates the cosine distance between the text vectors\n",
    "        Args:\n",
    "            text1 (str): column 1 data\n",
    "            text2 (str): column 2 data\n",
    "        \"\"\"\n",
    "        try:\n",
    "            tfidf_matrix = self.transformer(text1, text2)\n",
    "            return cosine_similarity(tfidf_matrix[0], tfidf_matrix[1])[0][0]\n",
    "        except:\n",
    "            return 0\n",
    "\n",
    "## Average runtime - 1s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a69d40a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_class = TFIDF_vectorizer(pd.concat([processed_df['name1'],\n",
    "                    processed_df['name2'],\n",
    "                    processed_df['short_description1'],\n",
    "                    processed_df['short_description2'],\n",
    "                    processed_df['long_description1'],\n",
    "                    processed_df['long_description2']],ignore_index=True))\n",
    "\n",
    "features_df = pd.DataFrame()\n",
    "\n",
    "# Calculate cosine similarity between name, short and long description columns of 2 products\n",
    "features_df['name_cos'] = processed_df.apply(lambda row: similarity_class.calculate_cosine_similarity(row['name1'], row['name2']), axis=1)\n",
    "features_df['short_description_cos'] = processed_df.apply(lambda row: similarity_class.calculate_cosine_similarity(row['short_description1'], row['short_description2']), axis=1)\n",
    "features_df['long_description_cos'] = processed_df.apply(lambda row: similarity_class.calculate_cosine_similarity(row['long_description1'], row['long_description2']), axis=1)\n",
    "features_df['all_texts_cos'] = processed_df.apply(lambda row: similarity_class.calculate_cosine_similarity(row['all_texts1'], row['all_texts2']), axis=1)\n",
    "\n",
    "## Average runtime - 35s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c1dbba",
   "metadata": {},
   "source": [
    "## 4.2 Keyword Detection and Similarity Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_sim(set1, set2):\n",
    "    \"\"\"\n",
    "    Jaccard similarity between two sets of keywords\n",
    "    Args:\n",
    "        set1 (list): words/tokens\n",
    "        set2 (list): words/tokens\n",
    "    \n",
    "    Return:\n",
    "        float: jaccard similarity\n",
    "    \"\"\"\n",
    "    \n",
    "    if not isinstance(set1, set):\n",
    "        set1 = set(set1)\n",
    "    \n",
    "    if not isinstance(set2, set):\n",
    "        set2 = set(set2)\n",
    "    \n",
    "    intersection = len(set1.intersection(set2))\n",
    "    union = len(set1.union(set2))\n",
    "    \n",
    "    return intersection / union if union != 0 else 0\n",
    "\n",
    "## Average runtime - 1s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.1 ID Detection\n",
    "\n",
    "* Selecting unique words longer than five characters that are not included in English vocab of ParaCrawl dataset\n",
    "* Extracted 3 features: name_id, short_description_id, all_texts_id\n",
    "\n",
    "Paracrawl:\n",
    "* Data dimensions: 9 GB, 50,632,000 lines\n",
    "* Created english vocab with 4,400,347 unique tokens of length more than 5 \n",
    "* Used english vocab from NLTK to deal with resource shortage for computations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens in Paracrawl English vocab: 4400347\n"
     ]
    }
   ],
   "source": [
    "# Vocab from paracrawl dataset\n",
    "# Run paracrawl.py file separately to create list of tokens\n",
    "english_vocab = open(\"features/english_words.txt\", encoding='utf-8').read().splitlines()\n",
    "english_vocab = [word.lower() for word in english_vocab if len(word)>5]\n",
    "print(f\"Number of tokens in Paracrawl English vocab: {len(english_vocab)}\")\n",
    "\n",
    "## Average runtime - 2s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "29942088",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLTK english corpus for computational efficiency\n",
    "english_vocab = [word for word in words.words() if len(word)>5]\n",
    "\n",
    "def detect_unique_ids(text, vocab=english_vocab):\n",
    "    \"\"\"\n",
    "    Detect unique words longer than five characters not in the vocabulary\n",
    "    Args:\n",
    "        text (str): string containing product information\n",
    "        vocab (list): list of tokens\n",
    "    \n",
    "    Return:\n",
    "        list: words that are not part of english vocab\n",
    "    \"\"\"\n",
    "\n",
    "    words = re.findall(r'\\b[\\w\\-.,\\'!$&*]{6,}\\b', text.lower())\n",
    "    return [word for word in words if word not in vocab]\n",
    "\n",
    "def calculate_id_detection(text1, text2, vocab=english_vocab):\n",
    "    \"\"\"\n",
    "    Calculate ID detection keywords similarity between two products \n",
    "    Args:\n",
    "        text1 (str): string containing product information\n",
    "        text2 (str): string containing product information\n",
    "        vocab (list): list of tokens\n",
    "        \n",
    "    Return:\n",
    "        int: jaccard similarity between the IDs\n",
    "    \"\"\"\n",
    "    set1 = detect_unique_ids(text1, vocab)\n",
    "    set2 = detect_unique_ids(text2, vocab)\n",
    "    \n",
    "    return jaccard_sim(set1, set2)\n",
    "\n",
    "def append_to_set(set_i,list_i):\n",
    "    set_i.update(list_i)\n",
    "    return set_i\n",
    "## Average runtime - 1s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eb04f750",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate cosine similarity between name, short and long description columns of 2 products\n",
    "features_df['name_id'] = processed_df.apply(lambda row: calculate_id_detection(row['name1'], row['name2']), axis=1)\n",
    "features_df['short_description_id'] = processed_df.apply(lambda row: calculate_id_detection(row['short_description1'], row['short_description2']), axis=1)\n",
    "features_df['all_texts_id'] = processed_df.apply(lambda row: calculate_id_detection(row['all_texts1'], row['all_texts2']), axis=1)\n",
    "\n",
    "## Average runtime - 15m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Generating list of all ids for both the products from all the attributes\n",
    "processed_df['ids_list1'] = processed_df.apply(lambda row: append_to_set(row.get('ids_list1',set()),detect_unique_ids(row['name1'])),axis=1)\n",
    "processed_df['ids_list1'] = processed_df.apply(lambda row: append_to_set(row.get('ids_list1',set()),detect_unique_ids(row['short_description1'])),axis=1)\n",
    "processed_df['ids_list1'] = processed_df.apply(lambda row: append_to_set(row.get('ids_list1',set()),detect_unique_ids(row['long_description1'])),axis=1)\n",
    "processed_df['ids_list1'] = processed_df.apply(lambda row: append_to_set(row.get('ids_list1',set()),detect_unique_ids(row['all_texts1'])),axis=1)\n",
    "processed_df['ids_list2'] = processed_df.apply(lambda row: append_to_set(row.get('ids_list2',set()),detect_unique_ids(row['name2'])),axis=1)\n",
    "processed_df['ids_list2'] = processed_df.apply(lambda row: append_to_set(row.get('ids_list2',set()),detect_unique_ids(row['short_description2'])),axis=1)\n",
    "processed_df['ids_list2'] = processed_df.apply(lambda row: append_to_set(row.get('ids_list2',set()),detect_unique_ids(row['long_description2'])),axis=1)\n",
    "processed_df['ids_list2'] = processed_df.apply(lambda row: append_to_set(row.get('ids_list2',set()),detect_unique_ids(row['all_texts2'])),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df['name_id']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b750998f",
   "metadata": {},
   "source": [
    "### 4.2.2 Brand Detections\n",
    "\n",
    "* Brand vocabulary created by processing the specification columns of source and target website\n",
    "* Extracted 3 features: name_brand, short_description_brand, all_texts_brand\n",
    "\n",
    "`REF` - https://stackoverflow.com/questions/5319922/check-if-a-word-is-in-a-string-in-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c4590a",
   "metadata": {},
   "outputs": [],
   "source": [
    "brand_vocab = text_processor.process_brand_names(\"specification1\")\n",
    "brand_vocab += text_processor.process_brand_names(\"specification2\")\n",
    "brand_vocab = set(brand_vocab)\n",
    "\n",
    "def findWholeWord(w):\n",
    "    return re.compile(r'\\b({0})\\b'.format(w), flags=re.IGNORECASE).search\n",
    "\n",
    "def detect_brands(text, brand_vocab=brand_vocab):\n",
    "    \"\"\"\n",
    "    Detect brands in the given text\n",
    "    Args:\n",
    "        text (str): string from the product information\n",
    "        brand_vocab (list): list of brands\n",
    "    \n",
    "    Returns:\n",
    "        list: brands detected in the product info\n",
    "    \"\"\"\n",
    "\n",
    "    return [word.lower() for word in brand_vocab if findWholeWord(word)(text)]\n",
    "\n",
    "\n",
    "def calculate_brand_detection(text1, text2, vocab=brand_vocab):\n",
    "    \"\"\"\n",
    "    Jaccard Similarity between the identified brands\n",
    "    Args:\n",
    "        text1 (str): string from the product information\n",
    "        text2 (str): string from the product information\n",
    "        brand_vocab (list): list of brands\n",
    "    \n",
    "    Returns:\n",
    "        int: Jaccard similarity between the brands of 2 products\n",
    "    \"\"\"\n",
    "    \n",
    "    set1 = detect_brands(text1, vocab)\n",
    "    set2 = detect_brands(text2, vocab)\n",
    "    \n",
    "    return jaccard_sim(set1, set2)\n",
    "\n",
    "## Average runtime - 1s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2412207",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df['name_brand'] = processed_df.apply(lambda row: calculate_brand_detection(row['name1'], row['name2']), axis=1)\n",
    "features_df['short_description_brand'] = processed_df.apply(lambda row: calculate_brand_detection(row['short_description1'], row['short_description2']), axis=1)\n",
    "features_df['all_texts_brand'] = processed_df.apply(lambda row: calculate_brand_detection(row['all_texts1'], row['all_texts2']), axis=1)\n",
    "\n",
    "## Average runtime - 1m 30s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_df['brand_list1'] = processed_df.apply(lambda row: append_to_set(row.get('brand_list1',set()),detect_brands(row['name1'])),axis=1)\n",
    "processed_df['brand_list1'] = processed_df.apply(lambda row: append_to_set(row.get('brand_list1',set()),detect_brands(row['short_description1'])),axis=1)\n",
    "processed_df['brand_list1'] = processed_df.apply(lambda row: append_to_set(row.get('brand_list1',set()),detect_brands(row['all_texts1'])),axis=1)\n",
    "processed_df['brand_list2'] = processed_df.apply(lambda row: append_to_set(row.get('brand_list2',set()),detect_brands(row['name2'])),axis=1)\n",
    "processed_df['brand_list2'] = processed_df.apply(lambda row: append_to_set(row.get('brand_list2',set()),detect_brands(row['short_description2'])),axis=1)\n",
    "processed_df['brand_list2'] = processed_df.apply(lambda row: append_to_set(row.get('brand_list2',set()),detect_brands(row['all_texts2'])),axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd0025ef",
   "metadata": {},
   "source": [
    "### 4.2.3 Number Detections\n",
    "\n",
    "* If no units are found near the number, the number is detected as a free number\n",
    "* Free numbers can contain model numbers or other crucial information\n",
    "* Extracted 5 features: name_numbers, short_description_numbers, long_description_numbers, specification_text_numbers, all_texts_numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a5511e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_numbers(text):\n",
    "    \"\"\"\n",
    "    Detect free numbers in the given text\n",
    "    Args:\n",
    "        text (str): string from the product information\n",
    "    \n",
    "    Returns:\n",
    "        list: free numbers detected in the product info\n",
    "    \"\"\"\n",
    "    \n",
    "    return [float(match.group()) for match in re.finditer(r'\\b\\d+(\\.\\d+)?\\b', text)]\n",
    "\n",
    "def calculate_numbers_detection(text1, text2):\n",
    "    \"\"\"\n",
    "    Jaccard Similarity between the identified free numbers \n",
    "    Args:\n",
    "        text1 (str): string from the product information\n",
    "        text2 (str): string from the product information\n",
    "    \n",
    "    Returns:\n",
    "        int: Jaccard similarity between the free numbers of 2 products\n",
    "    \"\"\"\n",
    "    \n",
    "    set1 = detect_numbers(text1)\n",
    "    set2 = detect_numbers(text2)\n",
    "    \n",
    "    return jaccard_sim(set1, set2)\n",
    "\n",
    "## Average runtime - 1s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b7c314",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df['name_numbers'] = processed_df.apply(lambda row: calculate_numbers_detection(row['name1'], row['name2']), axis=1)\n",
    "features_df['short_description_numbers'] = processed_df.apply(lambda row: calculate_numbers_detection(row['short_description1'], row['short_description2']), axis=1)\n",
    "features_df['long_description_numbers'] = processed_df.apply(lambda row: calculate_numbers_detection(row['long_description1'], row['long_description2']), axis=1)\n",
    "features_df['specification_text_numbers'] = processed_df.apply(lambda row: calculate_numbers_detection(row['specification1'], row['specification2']), axis=1)\n",
    "features_df['all_texts_numbers'] = processed_df.apply(lambda row: calculate_numbers_detection(row['all_texts1'], row['all_texts2']), axis=1)\n",
    "\n",
    "## Average runtime - 1s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_df['numbers_list1'] = processed_df.apply(lambda row: append_to_set(row.get('numbers_list1',set()),detect_numbers(row['name1'])),axis=1)\n",
    "processed_df['numbers_list1'] = processed_df.apply(lambda row: append_to_set(row.get('numbers_list1',set()),detect_numbers(row['short_description1'])),axis=1)\n",
    "processed_df['numbers_list1'] = processed_df.apply(lambda row: append_to_set(row.get('numbers_list1',set()),detect_numbers(row['long_description1'])),axis=1)\n",
    "processed_df['numbers_list1'] = processed_df.apply(lambda row: append_to_set(row.get('numbers_list1',set()),detect_numbers(row['all_texts1'])),axis=1)\n",
    "processed_df['numbers_list2'] = processed_df.apply(lambda row: append_to_set(row.get('numbers_list2',set()),detect_numbers(row['name2'])),axis=1)\n",
    "processed_df['numbers_list2'] = processed_df.apply(lambda row: append_to_set(row.get('numbers_list2',set()),detect_numbers(row['short_description2'])),axis=1)\n",
    "processed_df['numbers_list2'] = processed_df.apply(lambda row: append_to_set(row.get('numbers_list2',set()),detect_numbers(row['long_description2'])),axis=1)\n",
    "processed_df['numbers_list2'] = processed_df.apply(lambda row: append_to_set(row.get('numbers_list2',set()),detect_numbers(row['all_texts2'])),axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aefcdd56",
   "metadata": {},
   "source": [
    "### 4.2.4 Descriptive words\n",
    "\n",
    "* Set of the most characterising words for each attribute of the product\n",
    "* Extracted 4 features: name_descriptives, short_description_descriptives, long_description_descriptives, all_texts_descriptives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_word_document_frequency(word,documents):\n",
    "    counter = 0\n",
    "    for document in documents:\n",
    "        if word in document:\n",
    "            counter+=1\n",
    "    return counter\n",
    "\n",
    "def create_word_frequency_dict(dataframe, column):\n",
    "    words = dict()\n",
    "    for row in dataframe[column]:\n",
    "        for word in word_tokenize(row):\n",
    "            words[word] = words.get(word,calculate_word_document_frequency(word,dataframe[column]))\n",
    "    return words\n",
    "\n",
    "## Average runtime - 1s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name1_words_frequency_dict = create_word_frequency_dict(processed_df,'name1')\n",
    "name2_words_frequency_dict = create_word_frequency_dict(processed_df,'name2')\n",
    "short_description1_words_frequency_dict = create_word_frequency_dict(processed_df,'short_description1')\n",
    "short_description2_words_frequency_dict = create_word_frequency_dict(processed_df,'short_description2')\n",
    "long_description1_words_frequency_dict = create_word_frequency_dict(processed_df,'long_description1')\n",
    "long_description2_words_frequency_dict = create_word_frequency_dict(processed_df,'long_description2')\n",
    "all_texts1_words_frequency_dict = create_word_frequency_dict(processed_df,'all_texts1')\n",
    "all_texts2_words_frequency_dict = create_word_frequency_dict(processed_df,'all_texts2')\n",
    "\n",
    "## Average runtime - 23min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c0fd5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_descriptive_words(text,words_frequency_dict,documents_len,top_k = 50, maximum_p = 0.5):\n",
    "    \"\"\"\n",
    "    Detect descriptive words in the given text\n",
    "    Args:\n",
    "        text (str): string from the product information\n",
    "    \n",
    "    Returns:\n",
    "        list: descriptive words detected in the product info\n",
    "    \"\"\"\n",
    "    all_words =  [(word,words_frequency_dict[word]) for word in word_tokenize(text) if words_frequency_dict[word] < maximum_p*documents_len]\n",
    "    all_words.sort(key = lambda row: row[0])\n",
    "    return all_words[-top_k:]\n",
    "\n",
    "def calculate_descriptive_words(text1, text2,words_frequency_dict1,words_frequency_dict2,documents_len):\n",
    "    \"\"\"\n",
    "    Jaccard Similarity between the identified descriptive words \n",
    "    Args:\n",
    "        text1 (str): string from the product information\n",
    "        text2 (str): string from the product information\n",
    "    \n",
    "    Returns:\n",
    "        int: Jaccard similarity between the descriptive words of 2 products\n",
    "    \"\"\"\n",
    "    \n",
    "    set1 = detect_descriptive_words(text1,words_frequency_dict1,documents_len)\n",
    "    set2 = detect_descriptive_words(text2,words_frequency_dict2,documents_len)\n",
    "    \n",
    "    return jaccard_sim(set1, set2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents_len = len(processed_df)\n",
    "features_df['name_descriptives'] = processed_df.apply(lambda row: calculate_descriptive_words(row['name1'], row['name2'],name1_words_frequency_dict,name2_words_frequency_dict,documents_len), axis=1)\n",
    "\n",
    "features_df['short_description_descriptives'] = processed_df.apply(lambda row: calculate_descriptive_words(row['short_description1'], row['short_description2'],short_description1_words_frequency_dict,short_description2_words_frequency_dict,documents_len), axis=1)\n",
    "\n",
    "features_df['long_description_descriptives'] = processed_df.apply(lambda row: calculate_descriptive_words(row['long_description1'], row['long_description2'],long_description1_words_frequency_dict,long_description2_words_frequency_dict,documents_len), axis=1)\n",
    "\n",
    "features_df['all_texts_descriptives'] = processed_df.apply(lambda row: calculate_descriptive_words(row['all_texts1'], row['all_texts2'],all_texts1_words_frequency_dict,all_texts2_words_frequency_dict,documents_len), axis=1)\n",
    "\n",
    "## Average runtime - 4s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "712ffae5",
   "metadata": {},
   "source": [
    "### 4.2.5 Unit Detection\n",
    "\n",
    "* Extraction of numbers followed by units from each attribute \n",
    "* Extracted 5 features: name_units, short_description_units, long_description_units, specification_text_units, all_texts_units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21364e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_units(text):\n",
    "    \"\"\"\n",
    "    Detect numbers which are accompanied by units\n",
    "    Args:\n",
    "        text (str): string from the product information\n",
    "    \n",
    "    Returns:\n",
    "        list: numbers around units detected in the product info\n",
    "    \"\"\"\n",
    "    matches = re.findall(r'\\b(\\d+(\\.\\d+)?)\\s*([a-zA-Z]+)\\b', text)\n",
    "\n",
    "    return [match[0] for match in matches]\n",
    "\n",
    "def calculate_unit_detection(text1, text2):\n",
    "    \"\"\"\n",
    "    Jaccard Similarity between the identified descriptive words \n",
    "    Args:\n",
    "        text1 (str): string from the product information\n",
    "        text2 (str): string from the product information\n",
    "    \n",
    "    Returns:\n",
    "        int: Jaccard similarity between the detected numbers around units of 2 products\n",
    "    \"\"\"\n",
    "    set1 = detect_units(text1)\n",
    "    set2 = detect_units(text2)\n",
    "    \n",
    "    return jaccard_sim(set1, set2)\n",
    "\n",
    "## Average runtime - 1s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d9a049",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df['name_units'] = processed_df.apply(lambda row: calculate_unit_detection(row['name1'], row['name2']), axis=1)\n",
    "features_df['short_description_units'] = processed_df.apply(lambda row: calculate_unit_detection(row['short_description1'], row['short_description2']), axis=1)\n",
    "features_df['long_description_units'] = processed_df.apply(lambda row: calculate_unit_detection(row['long_description1'], row['long_description2']), axis=1)\n",
    "features_df['specification_text_units'] = processed_df.apply(lambda row: calculate_unit_detection(row['specification1'], row['specification2']), axis=1)\n",
    "features_df['all_texts_units'] = processed_df.apply(lambda row: calculate_unit_detection(row['all_texts1'], row['all_texts2']), axis=1)\n",
    "\n",
    "## Average runtime - 1s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Generating list of all units for both the products from all the attributes\n",
    "processed_df['units_list1'] = processed_df.apply(lambda row: append_to_set(row.get('units_list1',set()),detect_units(row['name1'])),axis=1)\n",
    "processed_df['units_list1'] = processed_df.apply(lambda row: append_to_set(row.get('units_list1',set()),detect_units(row['short_description1'])),axis=1)\n",
    "processed_df['units_list1'] = processed_df.apply(lambda row: append_to_set(row.get('units_list1',set()),detect_units(row['long_description1'])),axis=1)\n",
    "processed_df['units_list1'] = processed_df.apply(lambda row: append_to_set(row.get('units_list1',set()),detect_units(row['all_texts1'])),axis=1)\n",
    "processed_df['units_list2'] = processed_df.apply(lambda row: append_to_set(row.get('units_list2',set()),detect_units(row['name2'])),axis=1)\n",
    "processed_df['units_list2'] = processed_df.apply(lambda row: append_to_set(row.get('units_list2',set()),detect_units(row['short_description2'])),axis=1)\n",
    "processed_df['units_list2'] = processed_df.apply(lambda row: append_to_set(row.get('units_list2',set()),detect_units(row['long_description2'])),axis=1)\n",
    "processed_df['units_list2'] = processed_df.apply(lambda row: append_to_set(row.get('units_list2',set()),detect_units(row['all_texts2'])),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['name1', 'short_description1', 'long_description1', 'name2',\n",
       "       'short_description2', 'long_description2', 'specification1',\n",
       "       'specification2', 'all_texts1', 'all_texts2', 'orig_specification1',\n",
       "       'orig_specification2', 'match', 'brand_list1', 'brand_list2',\n",
       "       'numbers_list1', 'numbers_list2', 'units_list1', 'units_list2'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b6caa61",
   "metadata": {},
   "source": [
    "### 4.2.6 Words\n",
    "\n",
    "* Ratio of the same words taking all words from corresponding attributes of two products \n",
    "* Extracted 3 features: name_words, short_description_words, all_texts_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6210bbe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_words(text1, text2):\n",
    "    \"\"\"\n",
    "    Ratio of common words to all words \n",
    "    Args:\n",
    "        text1 (str): string from the product information\n",
    "        text2 (str): string from the product information\n",
    "    \n",
    "    Returns:\n",
    "        int: Jaccard similarity between the words of 2 products\n",
    "    \"\"\"\n",
    "    \n",
    "    set1 = set(word for word in text1.lower())\n",
    "    set2 = set(word for word in text2.lower())\n",
    "    \n",
    "    return jaccard_sim(set1, set2)\n",
    "\n",
    "## Average runtime - 1s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e8d4fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df['name_words'] = processed_df.apply(lambda row: calculate_words(row['name1'], row['name2']), axis=1)\n",
    "features_df['short_description_words'] = processed_df.apply(lambda row: calculate_words(row['short_description1'], row['short_description2']), axis=1)\n",
    "features_df['all_texts_words'] = processed_df.apply(lambda row: calculate_words(row['all_texts1'], row['all_texts2']), axis=1)\n",
    "\n",
    "## Average runtime - 1s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d56be07",
   "metadata": {},
   "source": [
    "### 4.3 All Detected Keywords Comparisons\n",
    "\n",
    "* Ratio of matching values in those lists between two compared products\n",
    "* Extracted 4 features: all_units_list, all_ids_list, all_numbers_list, all_brands_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb4f3172",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ratio(set1, set2):\n",
    "    \"\"\"\n",
    "    Ratio of common elements to total (can contain repetitions)\n",
    "    Args:\n",
    "        list1 (list): result from each type of detection\n",
    "        list2 (list): result from each type of detection\n",
    "    \n",
    "    Return:\n",
    "        int: common/total\n",
    "    \"\"\"\n",
    "    \n",
    "    total = len(set1.union(set2))\n",
    "    \n",
    "    return len(set1)/total if total != 0 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff88aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df['all_units_list'] = processed_df.apply(lambda row: ratio(row['units_list1'], row['units_list2']), axis=1)\n",
    "features_df['all_ids_list'] = processed_df.apply(lambda row: ratio(row['ids_list1'], row['ids_list2']), axis=1)\n",
    "features_df['all_numbers_list'] = processed_df.apply(lambda row: ratio(row['numbers_list1'], row['numbers_list2']), axis=1)\n",
    "features_df['all_brands_list'] = processed_df.apply(lambda row: ratio(row['brand_list1'], row['brand_list2']), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "268d6f68",
   "metadata": {},
   "source": [
    "### 4.4 Specification preprocessing\n",
    "\n",
    "* Ratio of corresponding parameter names as specification_key\n",
    "* Ratio of corresponding parameter names and values as specification_key_value\n",
    "* Extracted 2 features: specification_key, specification_key_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e576f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_key_value_match(text1, text2):\n",
    "    \"\"\"\n",
    "    Common key value pairs in the specification column\n",
    "    Args:\n",
    "        text1 (str): string from the product information\n",
    "        text2 (str): string from the product information\n",
    "    \n",
    "    Returns:\n",
    "        int: Ratio between common pairs and total pairs of the 2 products\n",
    "    \"\"\"\n",
    "    set1 = set([(d[\"key\"],d[\"value\"]) for d in eval(text1.lower())])\n",
    "    set2 = set([(d[\"key\"],d[\"value\"]) for d in eval(text2.lower())])\n",
    "\n",
    "    return jaccard_sim(set1,set2)\n",
    "\n",
    "def calculate_key_match(list1, list2):\n",
    "    \"\"\"\n",
    "    Common keys in the specification column\n",
    "    Args:\n",
    "        text1 (str): string from the product information\n",
    "        text2 (str): string from the product information\n",
    "    \n",
    "    Returns:\n",
    "        int: Ratio between common keys and total keys of the 2 products\n",
    "    \"\"\"\n",
    "    set1 = [d[\"key\"] for d in eval(list1.lower())]\n",
    "    set2 = [d[\"key\"] for d in eval(list2.lower())]\n",
    "\n",
    "    return jaccard_sim(set1, set2)\n",
    "\n",
    "## Average runtime - 1s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7adc3ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df['specification_key'] = processed_df.apply(lambda row: calculate_key_match(row['orig_specification1'], row['orig_specification2']), axis=1)\n",
    "features_df['specification_key_value'] = processed_df.apply(lambda row: calculate_key_value_match(row['orig_specification1'], row['orig_specification2']), axis=1)\n",
    "\n",
    "## Average runtime - 1s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37340ee7",
   "metadata": {},
   "source": [
    "### 4.5 Add Image Hash Similarities\n",
    "\n",
    "* Join the image hash similarity with text processing features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5592804",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1555, 1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hash_similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   hash_similarity\n",
       "0              0.0\n",
       "1              0.0\n",
       "2              0.0"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_hash_train = pd.read_csv(\"features/ProMapEn/images_train_similarties.csv\")\n",
    "image_hash_test = pd.read_csv(\"features/ProMapEn/images_test_similarties.csv\")\n",
    "\n",
    "image_hashes = pd.concat([image_hash_train, image_hash_test], ignore_index=True)\n",
    "\n",
    "print(image_hashes.shape)\n",
    "image_hashes.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "814c4a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df[\"hash_similarity\"] = image_hashes[\"hash_similarity\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d59aff",
   "metadata": {},
   "source": [
    "### 4.6 Label Stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e36cecb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "match\n",
       "0    1046\n",
       "1     509\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_df['match'] = processed_df[\"match\"]\n",
    "features_df['match'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b7ef15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1555, 19)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name_cos</th>\n",
       "      <th>short_description_cos</th>\n",
       "      <th>long_description_cos</th>\n",
       "      <th>all_texts_cos</th>\n",
       "      <th>name_brand</th>\n",
       "      <th>short_description_brand</th>\n",
       "      <th>all_texts_brand</th>\n",
       "      <th>name_numbers</th>\n",
       "      <th>short_description_numbers</th>\n",
       "      <th>long_description_numbers</th>\n",
       "      <th>specification_text_numbers</th>\n",
       "      <th>all_texts_numbers</th>\n",
       "      <th>all_units_list</th>\n",
       "      <th>all_numbers_list</th>\n",
       "      <th>all_brands_list</th>\n",
       "      <th>specification_key</th>\n",
       "      <th>specification_key_value</th>\n",
       "      <th>hash_similarity</th>\n",
       "      <th>match</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.230623</td>\n",
       "      <td>0.096354</td>\n",
       "      <td>0.194154</td>\n",
       "      <td>0.567405</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.312500</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.178245</td>\n",
       "      <td>0.299044</td>\n",
       "      <td>0.377834</td>\n",
       "      <td>0.717010</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.079730</td>\n",
       "      <td>0.549188</td>\n",
       "      <td>0.747834</td>\n",
       "      <td>0.787388</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.706158</td>\n",
       "      <td>0.385939</td>\n",
       "      <td>0.272149</td>\n",
       "      <td>0.715220</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.912291</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.346680</td>\n",
       "      <td>0.919648</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   name_cos  short_description_cos  long_description_cos  all_texts_cos  \\\n",
       "0  0.230623               0.096354              0.194154       0.567405   \n",
       "1  0.178245               0.299044              0.377834       0.717010   \n",
       "2  0.079730               0.549188              0.747834       0.787388   \n",
       "3  0.706158               0.385939              0.272149       0.715220   \n",
       "4  0.912291               0.000000              0.346680       0.919648   \n",
       "\n",
       "   name_brand  short_description_brand  all_texts_brand  name_numbers  \\\n",
       "0         1.0                      0.0              1.0      0.111111   \n",
       "1         0.0                      0.0              1.0      0.250000   \n",
       "2         0.0                      1.0              1.0      0.000000   \n",
       "3         1.0                      1.0              1.0      1.000000   \n",
       "4         1.0                      0.0              1.0      1.000000   \n",
       "\n",
       "   short_description_numbers  long_description_numbers  \\\n",
       "0                   0.000000                  0.000000   \n",
       "1                   0.666667                  0.333333   \n",
       "2                   0.500000                  0.000000   \n",
       "3                   0.100000                  0.000000   \n",
       "4                   0.000000                  0.200000   \n",
       "\n",
       "   specification_text_numbers  all_texts_numbers  all_units_list  \\\n",
       "0                         0.0           0.312500        0.571429   \n",
       "1                         0.0           0.400000        0.916667   \n",
       "2                         0.0           0.166667        0.750000   \n",
       "3                         0.0           0.307692        0.818182   \n",
       "4                         0.0           0.750000        1.000000   \n",
       "\n",
       "   all_numbers_list  all_brands_list  specification_key  \\\n",
       "0          0.625000              1.0                0.0   \n",
       "1          0.933333              1.0                0.0   \n",
       "2          0.846154              1.0                0.0   \n",
       "3          0.846154              1.0                0.0   \n",
       "4          1.000000              1.0                0.0   \n",
       "\n",
       "   specification_key_value  hash_similarity  match  \n",
       "0                      0.0              0.0      0  \n",
       "1                      0.0              0.0      0  \n",
       "2                      0.0              0.0      0  \n",
       "3                      0.0              0.0      1  \n",
       "4                      0.0              0.0      0  "
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(features_df.shape)\n",
    "features_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dbaa47e",
   "metadata": {},
   "source": [
    "## 5.0 Save the features into CSV file\n",
    "\n",
    "* Train set: 1244 rows\n",
    "* Test set: 311 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9687531",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the training data (1244, 19)\n",
      "Shape of the testing data (311, 19)\n"
     ]
    }
   ],
   "source": [
    "train_df = features_df.iloc[:1244, :]\n",
    "test_df = features_df.iloc[1244:, :]\n",
    "\n",
    "print(\"Shape of the training data\", train_df.shape)\n",
    "print(\"Shape of the testing data\", test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "819ff32e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.to_csv(\"features/ProMapEn/promapen_train_similarities.csv\", header=True, index=False)\n",
    "test_df.to_csv(\"features/ProMapEn/promapen_test_similarities.csv\", header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Reading the features again id \n",
    "\n",
    "# train_df = pd.read_csv(\"features/ProMapEn/promapen_train_similarities.csv\")\n",
    "# test_df = pd.read_csv(\"features/ProMapEn/promapen_test_similarities.csv\")\n",
    "# features_df = pd.concat([train_df, test_df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'all_ids_list',\n",
       " 'all_texts_descriptives',\n",
       " 'all_texts_units',\n",
       " 'all_texts_words',\n",
       " 'long_description_descriptives',\n",
       " 'long_description_units',\n",
       " 'name_descriptives',\n",
       " 'name_units',\n",
       " 'name_words',\n",
       " 'short_description_descriptives',\n",
       " 'short_description_units',\n",
       " 'short_description_words',\n",
       " 'specification_text_units'}"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columnslist = ['name_cos','long_description_descriptives','all_texts_cos','all_numbers_list','name_id','long_description_units','all_texts_brand','all_ids_list','name_brand','short_description_cos','all_texts_id','all_units_list','name_numbers','short_description_id','all_texts_numbers','specification_text_numbers','name_descriptives','short_description_brand','all_texts_descriptives','specification_text_units','name_units','short_description_numbers','all_texts_units','specification_key','name_words','short_description_descriptives','all_texts_words','specification_key_value','long_description_cos','short_description_units','all_brands_list','hash_similarity','long_description_numbers','short_description_words']\n",
    "\n",
    "set(columnslist) - set(features_df.columns)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
